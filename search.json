[
  {
    "objectID": "long_horizon.html",
    "href": "long_horizon.html",
    "title": "Long-Horizon Datasets",
    "section": "",
    "text": "source\n\nETTm2\n\n ETTm2 (freq:str='15T', name:str='ETTm2', n_ts:int=7, test_size:int=11520,\n        val_size:int=11520, horizons:Tuple[int]=(96, 192, 336, 720))\n\nThe ETTm2 dataset monitors an electricity transformer from a region of a province of China including oil temperature and variants of load (such as high useful load and high useless load) from July 2016 to July 2018 at a fifteen minute frequency.\nReference: Zhou, et al. Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting. AAAI 2021. https://arxiv.org/abs/2012.07436\n\nsource\n\n\nETTm1\n\n ETTm1 (freq:str='15T', name:str='ETTm1', n_ts:int=7, test_size:int=11520,\n        val_size:int=11520, horizons:Tuple[int]=(96, 192, 336, 720))\n\nThe ETTm1 dataset monitors an electricity transformer from a region of a province of China including oil temperature and variants of load (such as high useful load and high useless load) from July 2016 to July 2018 at a fifteen minute frequency.\n\nsource\n\n\nETTh2\n\n ETTh2 (freq:str='H', name:str='ETTh2', n_ts:int=1, test_size:int=11520,\n        val_size:int=11520, horizons:Tuple[int]=(96, 192, 336, 720))\n\nThe ETTh2 dataset monitors an electricity transformer from a region of a province of China including oil temperature and variants of load (such as high useful load and high useless load) from July 2016 to July 2018 at an hourly frequency.\n\nsource\n\n\nETTh1\n\n ETTh1 (freq:str='H', name:str='ETTh1', n_ts:int=1, test_size:int=11520,\n        val_size:int=11520, horizons:Tuple[int]=(96, 192, 336, 720))\n\nThe ETTh1 dataset monitors an electricity transformer from a region of a province of China including oil temperature and variants of load (such as high useful load and high useless load) from July 2016 to July 2018 at an hourly frequency.\n\nsource\n\n\nECL\n\n ECL (freq:str='15T', name:str='ECL', n_ts:int=321, test_size:int=5260,\n      val_size:int=2632, horizons:Tuple[int]=(96, 192, 336, 720))\n\nThe Electricity dataset reports the fifteen minute electricity consumption (KWh) of 321 customers from 2012 to 2014. For comparability, we aggregate it hourly.\nReference: Li, S et al. Enhancing the locality and breaking the memory bottleneck of Transformer on time series forecasting. NeurIPS 2019. http://arxiv.org/abs/1907.00235.\n\nsource\n\n\nExchange\n\n Exchange (freq:str='D', name:str='Exchange', n_ts:int=8,\n           test_size:int=1517, val_size:int=760, horizons:Tuple[int]=(96,\n           192, 336, 720))\n\nThe Exchange dataset is a collection of daily exchange rates of eight countries relative to the US dollar. The countries include Australia, UK, Canada, Switzerland, China, Japan, New Zealand and Singapore from 1990 to 2016.\nReference: Lai, G., Chang, W., Yang, Y., and Liu, H. Modeling Long and Short-Term Temporal Patterns with Deep Neural Networks. SIGIR 2018. http://arxiv.org/abs/1703.07015.\n\nsource\n\n\nTrafficL\n\n TrafficL (freq:str='H', name:str='traffic', n_ts:int=862,\n           test_size:int=3508, val_size:int=1756, horizons:Tuple[int]=(96,\n           192, 336, 720))\n\nThis large Traffic dataset was collected by the California Department of Transportation, it reports road hourly occupancy rates of 862 sensors, from January 2015 to December 2016.\nReference: Lai, G., Chang, W., Yang, Y., and Liu, H. Modeling Long and Short-Term Temporal Patterns with Deep Neural Networks. SIGIR 2018. http://arxiv.org/abs/1703.07015.\nWu, H., Xu, J., Wang, J., and Long, M. Autoformer: Decomposition Transformers with auto-correlation for long-term series forecasting. NeurIPS 2021. https://arxiv.org/abs/2106.13008.\n\nsource\n\n\nILI\n\n ILI (freq:str='W', name:str='ili', n_ts:int=7, test_size:int=193,\n      val_size:int=97, horizons:Tuple[int]=(24, 36, 48, 60))\n\nThis dataset reports weekly recorded influenza-like illness (ILI) patients from Centers for Disease Control and Prevention of the United States from 2002 to 2021. It is measured as a ratio of ILI patients versus the total patients in the week.\nReference: Wu, H., Xu, J., Wang, J., and Long, M. Autoformer: Decomposition Transformers with auto-correlation for long-term series forecasting. NeurIPS 2021. https://arxiv.org/abs/2106.13008.\n\nsource\n\n\nWeather\n\n Weather (freq:str='10M', name:str='weather', n_ts:int=21,\n          test_size:int=10539, val_size:int=5270, horizons:Tuple[int]=(96,\n          192, 336, 720))\n\nThis Weather dataset contains the 2020 year of 21 meteorological measurements recorded every 10 minutes from the Weather Station of the Max Planck Biogeochemistry Institute in Jena, Germany.\nReference: Wu, H., Xu, J., Wang, J., and Long, M. Autoformer: Decomposition Transformers with auto-correlation for long-term series forecasting. NeurIPS 2021. https://arxiv.org/abs/2106.13008.\n\nsource\n\n\nLongHorizon\n\n LongHorizon (source_url:str='https://nhits-\n              experiments.s3.amazonaws.com/datasets.zip')\n\nThis Long-Horizon datasets wrapper class, provides with utility to download and wrangle the following datasets:\nETT, ECL, Exchange, Traffic, ILI and Weather.\n\nEach set is normalized with the train data mean and standard deviation.\nDatasets are partitioned into train, validation and test splits.\nFor all datasets: 70%, 10%, and 20% of observations are train, validation, test, except ETT that uses 20% validation."
  },
  {
    "objectID": "m3.html",
    "href": "m3.html",
    "title": "M3 dataset",
    "section": "",
    "text": "M3 meta information\n\n\nsource\n\nOther\n\n Other (seasonality:int=1, horizon:int=8, freq:str='D',\n        sheet_name:str='M3Other', name:str='Other', n_ts:int=174)\n\n\nsource\n\n\nMonthly\n\n Monthly (seasonality:int=12, horizon:int=18, freq:str='M',\n          sheet_name:str='M3Month', name:str='Monthly', n_ts:int=1428)\n\n\nsource\n\n\nQuarterly\n\n Quarterly (seasonality:int=4, horizon:int=8, freq:str='Q',\n            sheet_name:str='M3Quart', name:str='Quarterly', n_ts:int=756)\n\n\nsource\n\n\nYearly\n\n Yearly (seasonality:int=1, horizon:int=6, freq:str='Y',\n         sheet_name:str='M3Year', name:str='Yearly', n_ts:int=645)\n\n\nsource\n\n\nM3\n\n M3 ()"
  },
  {
    "objectID": "losses.html",
    "href": "losses.html",
    "title": "NumPy Evaluation",
    "section": "",
    "text": "source\n\n\n\n mae (y:numpy.ndarray, y_hat:numpy.ndarray,\n      weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nCalculates Mean Absolute Error (MAE) between y and y_hat. MAE measures the relative prediction accuracy of a forecasting method by calculating the deviation of the prediction and the true value at a given time and averages these devations over the length of the series.\n\\[ \\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) =\n    \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1}\n    |y_{\\tau} - \\hat{y}_{\\tau}| \\]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\n\n\n\ny_hat\nndarray\n\n\n\n\nweights\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\naxis\ntyping.Optional[int]\nNone\n\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\n\nReturn the MAE along the specified axis.\n\n\n\n\n\n\n\n\nsource\n\n\n\n mse (y:numpy.ndarray, y_hat:numpy.ndarray,\n      weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nCalculates Mean Squared Error (MSE) between y and y_hat. MSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the true value at a given time, and averages these devations over the length of the series.\n\\[ \\mathrm{MSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) =\n    \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2} \\]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\n\n\n\ny_hat\nndarray\n\n\n\n\nweights\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\naxis\ntyping.Optional[int]\nNone\n\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\n\nReturn the MSE along the specified axis.\n\n\n\n\n\n\n\n\nsource\n\n\n\n rmse (y:numpy.ndarray, y_hat:numpy.ndarray,\n       weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nCalculates Root Mean Squared Error (RMSE) between y and y_hat. RMSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the observed value at a given time and averages these devations over the length of the series. Finally the RMSE will be in the same scale as the original time series so its comparison with other series is possible only if they share a common scale. RMSE has a direct connection to the L2 norm.\n\\[ \\mathrm{RMSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) =\n    \\sqrt{\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2}} \\]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\n\n\n\ny_hat\nndarray\n\n\n\n\nweights\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\naxis\ntyping.Optional[int]\nNone\n\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\n\nReturn the RMSE along the specified axis."
  },
  {
    "objectID": "losses.html#mean-absolute-error",
    "href": "losses.html#mean-absolute-error",
    "title": "NumPy Evaluation",
    "section": "",
    "text": "source\n\n\n\n mae (y:numpy.ndarray, y_hat:numpy.ndarray,\n      weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nCalculates Mean Absolute Error (MAE) between y and y_hat. MAE measures the relative prediction accuracy of a forecasting method by calculating the deviation of the prediction and the true value at a given time and averages these devations over the length of the series.\n\\[ \\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) =\n    \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1}\n    |y_{\\tau} - \\hat{y}_{\\tau}| \\]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\n\n\n\ny_hat\nndarray\n\n\n\n\nweights\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\naxis\ntyping.Optional[int]\nNone\n\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\n\nReturn the MAE along the specified axis."
  },
  {
    "objectID": "losses.html#mean-squared-error",
    "href": "losses.html#mean-squared-error",
    "title": "NumPy Evaluation",
    "section": "",
    "text": "source\n\n\n\n mse (y:numpy.ndarray, y_hat:numpy.ndarray,\n      weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nCalculates Mean Squared Error (MSE) between y and y_hat. MSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the true value at a given time, and averages these devations over the length of the series.\n\\[ \\mathrm{MSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) =\n    \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2} \\]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\n\n\n\ny_hat\nndarray\n\n\n\n\nweights\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\naxis\ntyping.Optional[int]\nNone\n\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\n\nReturn the MSE along the specified axis."
  },
  {
    "objectID": "losses.html#root-mean-squared-error",
    "href": "losses.html#root-mean-squared-error",
    "title": "NumPy Evaluation",
    "section": "",
    "text": "source\n\n\n\n rmse (y:numpy.ndarray, y_hat:numpy.ndarray,\n       weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nCalculates Root Mean Squared Error (RMSE) between y and y_hat. RMSE measures the relative prediction accuracy of a forecasting method by calculating the squared deviation of the prediction and the observed value at a given time and averages these devations over the length of the series. Finally the RMSE will be in the same scale as the original time series so its comparison with other series is possible only if they share a common scale. RMSE has a direct connection to the L2 norm.\n\\[ \\mathrm{RMSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) =\n    \\sqrt{\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2}} \\]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\n\n\n\ny_hat\nndarray\n\n\n\n\nweights\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\naxis\ntyping.Optional[int]\nNone\n\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\n\nReturn the RMSE along the specified axis."
  },
  {
    "objectID": "losses.html#mean-absolute-percentage-error",
    "href": "losses.html#mean-absolute-percentage-error",
    "title": "NumPy Evaluation",
    "section": "Mean Absolute Percentage Error",
    "text": "Mean Absolute Percentage Error\n\nsource\n\nmape\n\n mape (y:numpy.ndarray, y_hat:numpy.ndarray,\n       weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nCalculates Mean Absolute Percentage Error (MAPE) between y and y_hat. MAPE measures the relative prediction accuracy of a forecasting method by calculating the percentual deviation of the prediction and the observed value at a given time and averages these devations over the length of the series. The closer to zero an observed value is, the higher penalty MAPE loss assigns to the corresponding error.\n\\[ \\mathrm{MAPE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) =\n    \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1}\n    \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|} \\]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\n\n\n\ny_hat\nndarray\n\n\n\n\nweights\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\naxis\ntyping.Optional[int]\nNone\n\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\n\nReturn the MAPE along the specified axis."
  },
  {
    "objectID": "losses.html#symmetric-mean-absolute-percentage-error",
    "href": "losses.html#symmetric-mean-absolute-percentage-error",
    "title": "NumPy Evaluation",
    "section": "Symmetric Mean Absolute Percentage Error",
    "text": "Symmetric Mean Absolute Percentage Error\n\nsource\n\nsmape\n\n smape (y:numpy.ndarray, y_hat:numpy.ndarray,\n        weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nCalculates Symmetric Mean Absolute Percentage Error (SMAPE) between y and y_hat. SMAPE measures the relative prediction accuracy of a forecasting method by calculating the relative deviation of the prediction and the observed value scaled by the sum of the absolute values for the prediction and observed value at a given time, then averages these devations over the length of the series. This allows the SMAPE to have bounds between 0% and 200% which is desireble compared to normal MAPE that may be undetermined when the target is zero.\n\\[ \\mathrm{SMAPE}_{2}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) =\n   \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1}\n   \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|+|\\hat{y}_{\\tau}|} \\]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\n\n\n\ny_hat\nndarray\n\n\n\n\nweights\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\naxis\ntyping.Optional[int]\nNone\n\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\n\nReturn the SMAPE along the specified axis."
  },
  {
    "objectID": "losses.html#mean-absolute-scaled-error",
    "href": "losses.html#mean-absolute-scaled-error",
    "title": "NumPy Evaluation",
    "section": "Mean Absolute Scaled Error",
    "text": "Mean Absolute Scaled Error\n/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/fastcore/docscrape.py:225: UserWarning: Unknown section References\n  else: warn(msg)\n\nsource\n\nmase\n\n mase (y:numpy.ndarray, y_hat:numpy.ndarray, y_train:numpy.ndarray,\n       seasonality:int, weights:Optional[numpy.ndarray]=None,\n       axis:Optional[int]=None)\n\nCalculates the Mean Absolute Scaled Error (MASE) between y and y_hat. MASE measures the relative prediction accuracy of a forecasting method by comparinng the mean absolute errors of the prediction and the observed value against the mean absolute errors of the seasonal naive model. The MASE partially composed the Overall Weighted Average (OWA), used in the M4 Competition.\n\\[ \\mathrm{MASE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau}) =\n    \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau})} \\]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\n\n\n\ny_hat\nndarray\n\n\n\n\ny_train\nndarray\n\n\n\n\nseasonality\nint\n\n\n\n\nweights\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\naxis\ntyping.Optional[int]\nNone\n\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\n\nReturn the mase along the specified axis."
  },
  {
    "objectID": "losses.html#relative-mean-absolute-error",
    "href": "losses.html#relative-mean-absolute-error",
    "title": "NumPy Evaluation",
    "section": "Relative Mean Absolute Error",
    "text": "Relative Mean Absolute Error\n\nsource\n\nrmae\n\n rmae (y:numpy.ndarray, y_hat1:numpy.ndarray, y_hat2:numpy.ndarray,\n       weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nCalculates Relative Mean Absolute Error (RMAE) between two sets of forecasts (from two different forecasting methods). A number smaller than one implies that the forecast in the numerator is better than the forecast in the denominator.\n\\[ \\mathrm{RMAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{base}_{\\tau}) =\n    \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{base}_{\\tau})} \\]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\n\n\n\ny_hat1\nndarray\n\n\n\n\ny_hat2\nndarray\n\n\n\n\nweights\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\naxis\ntyping.Optional[int]\nNone\n\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\n\nReturn the RMAE along the specified axis."
  },
  {
    "objectID": "losses.html#quantile-loss",
    "href": "losses.html#quantile-loss",
    "title": "NumPy Evaluation",
    "section": "Quantile Loss",
    "text": "Quantile Loss\n\nsource\n\nquantile_loss\n\n quantile_loss (y:numpy.ndarray, y_hat:numpy.ndarray, q:float=0.5,\n                weights:Optional[numpy.ndarray]=None,\n                axis:Optional[int]=None)\n\nComputes the quantile loss (QL) between y and y_hat. QL measures the deviation of a quantile forecast. By weighting the absolute deviation in a non symmetric way, the loss pays more attention to under or over estimation.\nA common value for q is 0.5 for the deviation from the median.\n\\[ \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) =\n    \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1}\n    \\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\tau} - y_{\\tau} )_{+}\n    + q\\,( y_{\\tau} - \\hat{y}^{(q)}_{\\tau} )_{+} \\Big) \\]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\n\n\n\ny_hat\nndarray\n\n\n\n\nq\nfloat\n0.5\n\n\n\nweights\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\naxis\ntyping.Optional[int]\nNone\n\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\n\nReturn the QL along the specified axis."
  },
  {
    "objectID": "losses.html#multi-quantile-loss",
    "href": "losses.html#multi-quantile-loss",
    "title": "NumPy Evaluation",
    "section": "Multi-Quantile Loss",
    "text": "Multi-Quantile Loss\n\nsource\n\nmqloss\n\n mqloss (y:numpy.ndarray, y_hat:numpy.ndarray, quantiles:numpy.ndarray,\n         weights:Optional[numpy.ndarray]=None, axis:Optional[int]=None)\n\nCalculates the Multi-Quantile loss (MQL) between y and y_hat. MQL calculates the average multi-quantile Loss for a given set of quantiles, based on the absolute difference between predicted quantiles and observed values.\n\\[ \\mathrm{MQL}(\\mathbf{y}_{\\tau},\n                [\\mathbf{\\hat{y}}^{(q_{1})}_{\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\tau}]) =\n   \\frac{1}{n} \\sum_{q_{i}} \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q_{i})}_{\\tau}) \\]\nThe limit behavior of MQL allows to measure the accuracy of a full predictive distribution \\(\\mathbf{\\hat{F}}_{\\tau}\\) with the continuous ranked probability score (CRPS). This can be achieved through a numerical integration technique, that discretizes the quantiles and treats the CRPS integral with a left Riemann approximation, averaging over uniformly distanced quantiles.\n\\[ \\mathrm{CRPS}(y_{\\tau}, \\mathbf{\\hat{F}}_{\\tau}) =\n    \\int^{1}_{0} \\mathrm{QL}(y_{\\tau}, \\hat{y}^{(q)}_{\\tau}) dq \\]\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\n\n\n\ny_hat\nndarray\n\n\n\n\nquantiles\nndarray\n\n\n\n\nweights\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\naxis\ntyping.Optional[int]\nNone\n\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\n\nReturn the MQL along the specified axis."
  },
  {
    "objectID": "losses.html#coverage",
    "href": "losses.html#coverage",
    "title": "NumPy Evaluation",
    "section": "Coverage",
    "text": "Coverage\n\nsource\n\ncoverage\n\n coverage (y:numpy.ndarray, y_hat_lo:numpy.ndarray,\n           y_hat_hi:numpy.ndarray)\n\nCalculates the coverage of y with y_hat_lo and y_hat_hi.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ny\nndarray\n\n\n\ny_hat_lo\nndarray\n\n\n\ny_hat_hi\nndarray\n\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\nReturn the coverage of y_hat."
  },
  {
    "objectID": "losses.html#calibration",
    "href": "losses.html#calibration",
    "title": "NumPy Evaluation",
    "section": "Calibration",
    "text": "Calibration\n\nsource\n\ncalibration\n\n calibration (y:numpy.ndarray, y_hat_hi:numpy.ndarray)\n\nCalculates the fraction of y that is lower than y_hat_hi.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ny\nndarray\n\n\n\ny_hat_hi\nndarray\n\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\nReturn the coverage of y_hat."
  },
  {
    "objectID": "losses.html#crps",
    "href": "losses.html#crps",
    "title": "NumPy Evaluation",
    "section": "CRPS",
    "text": "CRPS\n\nsource\n\nscaled_crps\n\n scaled_crps (y:numpy.ndarray, y_hat:numpy.ndarray,\n              quantiles:numpy.ndarray,\n              weights:Optional[numpy.ndarray]=None,\n              axis:Optional[int]=None)\n\nScaled Continues Ranked Probability Score\nCalculates a scaled variation of the CRPS, as proposed by Rangapuram (2021), to measure the accuracy of predicted quantiles y_hat compared to the observation y. This metric averages percentual weighted absolute deviations as defined by the quantile losses.\n\\[ \\mathrm{sCRPS}(\\hat{F}_{\\tau}, \\mathbf{y}_{\\tau}) = \\frac{2}{N} \\sum_{i}\n\\int^{1}_{0}\n\\frac{\\mathrm{QL}(\\hat{F}_{i,\\tau}, y_{i,\\tau})_{q}}{\\sum_{i} | y_{i,\\tau} |} dq \\]\nWhere \\(\\hat{F}_{\\tau}\\) is the an estimated multivariate distribution, and \\(y_{i,\\tau}\\) are its realizations.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ny\nndarray\n\n\n\n\ny_hat\nndarray\n\n\n\n\nquantiles\nndarray\n\n\n\n\nweights\ntyping.Optional[numpy.ndarray]\nNone\n\n\n\naxis\ntyping.Optional[int]\nNone\n\n\n\nReturns\ntyping.Union[float, numpy.ndarray]\n\nReturn the scaled crps along the specified axis."
  },
  {
    "objectID": "m5.html",
    "href": "m5.html",
    "title": "M5 dataset",
    "section": "",
    "text": "source\n\n\n\n M5 (source_url:str='https://github.com/Nixtla/m5-forecasts/raw/main/datas\n     ets/m5.zip')"
  },
  {
    "objectID": "m5.html#download-data-class",
    "href": "m5.html#download-data-class",
    "title": "M5 dataset",
    "section": "",
    "text": "source\n\n\n\n M5 (source_url:str='https://github.com/Nixtla/m5-forecasts/raw/main/datas\n     ets/m5.zip')"
  },
  {
    "objectID": "m5.html#test-number-of-series",
    "href": "m5.html#test-number-of-series",
    "title": "M5 dataset",
    "section": "Test number of series",
    "text": "Test number of series"
  },
  {
    "objectID": "m5.html#evaluation-class",
    "href": "m5.html#evaluation-class",
    "title": "M5 dataset",
    "section": "Evaluation class",
    "text": "Evaluation class\n\nsource\n\nM5Evaluation\n\n M5Evaluation ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nURL-based evaluation\nThe method evaluate from the class M5Evaluation can receive a url of a submission to the M5 competiton.\nThe results compared to the on-the-fly evaluation were obtained from the official evaluation.\n\nm5_winner_url = 'https://github.com/Nixtla/m5-forecasts/raw/main/forecasts/0001 YJ_STU.zip'\nwinner_evaluation = M5Evaluation.evaluate('data', m5_winner_url)\n# Test of the same evaluation as the original one\ntest_close(winner_evaluation.loc['Total'].item(), 0.520, eps=1e-3)\nwinner_evaluation\n\n\n\nPandas-based evaluation\nAlso the method evaluate can recevie a pandas DataFrame of forecasts.\n\nm5_second_place_url = 'https://github.com/Nixtla/m5-forecasts/raw/main/forecasts/0002 Matthias.zip'\nm5_second_place_forecasts = M5Evaluation.load_benchmark('data', m5_second_place_url)\nsecond_place_evaluation = M5Evaluation.evaluate('data', m5_second_place_forecasts)\n# Test of the same evaluation as the original one\ntest_close(second_place_evaluation.loc['Total'].item(), 0.528, eps=1e-3)\nsecond_place_evaluation\n\nBy default you can load the winner benchmark using the following.\n\nwinner_benchmark = M5Evaluation.load_benchmark('data')\nwinner_evaluation = M5Evaluation.evaluate('data', winner_benchmark)\n# Test of the same evaluation as the original one\ntest_close(winner_evaluation.loc['Total'].item(), 0.520, eps=1e-3)\nwinner_evaluation\n\n\n\nValidation evaluation\nYou can also evaluate the official validation set.\n\nwinner_benchmark_val = M5Evaluation.load_benchmark('data', validation=True)\nwinner_evaluation_val = M5Evaluation.evaluate('data', winner_benchmark_val, validation=True)\nwinner_evaluation_val"
  },
  {
    "objectID": "m5.html#kaggle-competition-m5-references",
    "href": "m5.html#kaggle-competition-m5-references",
    "title": "M5 dataset",
    "section": "Kaggle-Competition-M5 References",
    "text": "Kaggle-Competition-M5 References\nThe evaluation metric of the Favorita Kaggle competition was the normalized weighted root mean squared logarithmic error (NWRMSLE). Perishable items have a score weight of 1.25; otherwise, the weight is 1.0.\n\\[ NWRMSLE = \\sqrt{\\frac{\\sum^{n}_{i=1} w_{i}\\left(log(\\hat{y}_{i}+1)  - log(y_{i}+1)\\right)^{2}}{\\sum^{n}_{i=1} w_{i}}}\\]\n\n\n\nKaggle Competition Forecasting Methods\n16D ahead NWRMSLE\n\n\n\n\nLGBM [1]\n0.5091\n\n\nSeq2Seq WaveNet [2]\n0.5129\n\n\n\n\nCorporación Favorita. Corporación favorita grocery sales forecasting. Kaggle Competition Leaderboard, 2018.\nGlib Kechyn, Lucius Yu, Yangguang Zang, and Svyatoslav Kechyn. Sales forecasting using wavenet within the framework of the Favorita Kaggle competition. Computing Research Repository, abs/1803.04037, 2018."
  },
  {
    "objectID": "evaluation.html",
    "href": "evaluation.html",
    "title": "Datasets Evaluation",
    "section": "",
    "text": "source\n\naccuracy\n\n accuracy (Y_hat_df:fugue.dataframe.dataframe.DataFrame,\n           metrics:List[Callable],\n           Y_test_df:Optional[fugue.dataframe.dataframe.DataFrame]=None,\n           Y_df:Optional[fugue.dataframe.dataframe.DataFrame]=None,\n           id_col:str='unique_id', time_col:str='ds', target_col:str='y',\n           level:Optional[List]=None, agg_by:Optional[List[str]]=None,\n           agg_fn:Callable=&lt;function mean at 0x7fe26a1572b0&gt;)\n\nEvaluate forecast using different metrics.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nY_hat_df\nDataFrame\n\nForecasts and models to evaluate.Can contain the actual values given by target_col.\n\n\nmetrics\ntyping.List[typing.Callable]\n\nFunctions with arguments y, y_hat, and optionally y_train.\n\n\nY_test_df\ntyping.Optional[fugue.dataframe.dataframe.DataFrame]\nNone\nTrue values. Nedded if Y_hat_df does not have the true values.\n\n\nY_df\ntyping.Optional[fugue.dataframe.dataframe.DataFrame]\nNone\nTraining set. Used to evaluate metrics such as mase.\n\n\nid_col\nstr\nunique_id\nColumn that identifies each serie. If ‘index’ then the index is used.\n\n\ntime_col\nstr\nds\nColumn that identifies each timestep, its values can be timestamps or integers.\n\n\ntarget_col\nstr\ny\nColumn that contains the target.\n\n\nlevel\ntyping.Optional[typing.List]\nNone\n\n\n\nagg_by\ntyping.Optional[typing.List[str]]\nNone\n\n\n\nagg_fn\ntyping.Callable\nmean\n\n\n\nReturns\nDataFrame\n\nMetrics with one column per model."
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Datasets Utils",
    "section": "",
    "text": "source\n\ndownload_file\n\n download_file (directory:str, source_url:str, decompress:bool=False)\n\nDownload data from source_ulr inside directory.\n\nsource\n\n\nextract_file\n\n extract_file (filepath, directory)\n\n\nsource\n\n\nasync_download_files\n\n async_download_files (path:Union[str,pathlib.Path], urls:Iterable[str])\n\n\nimport tempfile\n\nimport requests\n\n\ngh_url = 'https://api.github.com/repos/Nixtla/datasetsforecast/git/trees/main'\nbase_url = 'https://raw.githubusercontent.com/Nixtla/datasetsforecast/main'\n\nresp = requests.get(gh_url)\nurls = []\nfor e in resp.json()['tree']:\n    if e['type'] == 'blob':\n        urls.append(f'{base_url}/{e[\"path\"]}')\nwith tempfile.TemporaryDirectory() as tmp:\n    tmp = Path(tmp)\n    await async_download_files(tmp, urls)\n    files = list(tmp.iterdir())\n    assert len(files) == len(urls)\n\n\nsource\n\n\ndownload_files\n\n download_files (directory:Union[str,pathlib.Path], urls:Iterable[str])\n\n\nwith tempfile.TemporaryDirectory() as tmp:\n    tmp = Path(tmp)\n    fname = tmp / 'script.py'\n    fname.write_text(f\"\"\"\nfrom datasetsforecast.utils import download_files\n    \ndownload_files('{tmp.as_posix()}', {urls})\n    \"\"\")\n    !python {fname}\n    fname.unlink()\n    files = list(tmp.iterdir())\n    assert len(files) == len(urls)\n\n\nsource\n\n\nInfo\n\n Info (class_groups:Tuple[dataclass])\n\nInfo Dataclass of datasets. Args: groups (Tuple): Tuple of str groups class_groups (Tuple): Tuple of dataclasses."
  },
  {
    "objectID": "hierarchical.html",
    "href": "hierarchical.html",
    "title": "Hierarchical Datasets",
    "section": "",
    "text": "Here we host a collection of datasets used in previous hierarchical research by Rangapuram et al. [2021], Olivares et al. [2023], and Kamarthi et al. [2022]. The benchmark datasets utilized include Australian Monthly Labour (Labour), SF Bay Area daily Traffic (Traffic, OldTraffic), Quarterly Australian Tourism Visits (TourismSmall), Monthly Australian Tourism visits (TourismLarge, OldTourismLarge), and daily Wikipedia article views (Wiki2). Old datasets favor the original datasets with minimal target variable preprocessing (Rangapuram et al. [2021], Olivares et al. [2023]), while the remaining datasets follow PROFHIT experimental settings."
  },
  {
    "objectID": "hierarchical.html#references",
    "href": "hierarchical.html#references",
    "title": "Hierarchical Datasets",
    "section": "References",
    "text": "References\n\nSyama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus, Tim Januschowski. (2021). “End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series”. Proceedings of the 38th International Conference on Machine Learning (ICML).\nKin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker (2022).”Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures”. International Journal Forecasting, special issue.\nHarshavardhan Kamarthi, Lingkai Kong, Alexander Rodriguez, Chao Zhang, and B. Prakash. PROFHIT: Probabilistic robust forecasting for hierarchical time-series. Computing Research Repository.URL https://arxiv.org/abs/2206.07940.\n\n\nsource\n\nLabour\n\n Labour (freq:str='MS', horizon:int=8, papers_horizon:int=12,\n         seasonality:int=12, test_size:int=125,\n         tags_names:Tuple[str]=('Country', 'Country/Region',\n         'Country/Gender/Region', 'Country/Employment/Gender/Region'))\n\n\nsource\n\n\nTourismLarge\n\n TourismLarge (freq:str='MS', horizon:int=12, papers_horizon:int=12,\n               seasonality:int=12, test_size:int=57,\n               tags_names:Tuple[str]=('Country', 'Country/State',\n               'Country/State/Zone', 'Country/State/Zone/Region',\n               'Country/Purpose', 'Country/State/Purpose',\n               'Country/State/Zone/Purpose',\n               'Country/State/Zone/Region/Purpose'))\n\n\nsource\n\n\nTourismSmall\n\n TourismSmall (freq:str='Q', horizon:int=4, papers_horizon:int=4,\n               seasonality:int=4, test_size:int=9,\n               tags_names:Tuple[str]=('Country', 'Country/Purpose',\n               'Country/Purpose/State',\n               'Country/Purpose/State/CityNonCity'))\n\n\nsource\n\n\nTraffic\n\n Traffic (freq:str='D', horizon:int=14, papers_horizon:int=7,\n          seasonality:int=7, test_size:int=91,\n          tags_names:Tuple[str]=('Level1', 'Level2', 'Level3', 'Level4'))\n\n\nsource\n\n\nWiki2\n\n Wiki2 (freq:str='D', horizon:int=14, papers_horizon:int=7,\n        seasonality:int=7, test_size:int=91,\n        tags_names:Tuple[str]=('Views', 'Views/Country',\n        'Views/Country/Access', 'Views/Country/Access/Agent',\n        'Views/Country/Access/Agent/Topic'))\n\n\nsource\n\n\nOldTraffic\n\n OldTraffic (freq:str='D', horizon:int=1, papers_horizon:int=1,\n             seasonality:int=7, test_size:int=91,\n             tags_names:Tuple[str]=('Level1', 'Level2', 'Level3',\n             'Level4'))\n\n\nsource\n\n\nOldTourismLarge\n\n OldTourismLarge (freq:str='MS', horizon:int=12, papers_horizon:int=12,\n                  seasonality:int=12, test_size:int=57,\n                  tags_names:Tuple[str]=('Country', 'Country/State',\n                  'Country/State/Zone', 'Country/State/Zone/Region',\n                  'Country/Purpose', 'Country/State/Purpose',\n                  'Country/State/Zone/Purpose',\n                  'Country/State/Zone/Region/Purpose'))\n\n\nsource\n\n\nHierarchicalData\n\n HierarchicalData ()\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "long_horizon2.html",
    "href": "long_horizon2.html",
    "title": "Long-Horizon Original Datasets",
    "section": "",
    "text": "source\n\nWeather\n\n Weather (freq:str='10M', name:str='weather', n_ts:int=21,\n          test_size:int=10539, val_size:int=5270, horizons:Tuple[int]=(96,\n          192, 336, 720))\n\nThis Weather dataset contains the 2020 year of 21 meteorological measurements recorded every 10 minutes from the Weather Station of the Max Planck Biogeochemistry Institute in Jena, Germany.\nReference: Wu, H., Xu, J., Wang, J., and Long, M. Autoformer: Decomposition Transformers with auto-correlation for long-term series forecasting. NeurIPS 2021. https://arxiv.org/abs/2106.13008.\n\nsource\n\n\nTrafficL\n\n TrafficL (freq:str='H', name:str='traffic', n_ts:int=862,\n           test_size:int=3508, val_size:int=1756, horizons:Tuple[int]=(96,\n           192, 336, 720))\n\nThis large Traffic dataset was collected by the California Department of Transportation, it reports road hourly occupancy rates of 862 sensors, from January 2015 to December 2016.\nReference: Lai, G., Chang, W., Yang, Y., and Liu, H. Modeling Long and Short-Term Temporal Patterns with Deep Neural Networks. SIGIR 2018. http://arxiv.org/abs/1703.07015.\nWu, H., Xu, J., Wang, J., and Long, M. Autoformer: Decomposition Transformers with auto-correlation for long-term series forecasting. NeurIPS 2021. https://arxiv.org/abs/2106.13008.\n\nsource\n\n\nECL\n\n ECL (freq:str='15T', name:str='ECL', n_ts:int=321, n_time:int=26304,\n      test_size:int=5260, val_size:int=2632, horizons:Tuple[int]=(96, 192,\n      336, 720))\n\nThe Electricity dataset reports the fifteen minute electricity consumption (KWh) of 321 customers from 2012 to 2014. For comparability, we aggregate it hourly.\nReference: Li, S et al. Enhancing the locality and breaking the memory bottleneck of Transformer on time series forecasting. NeurIPS 2019. http://arxiv.org/abs/1907.00235.\n\nsource\n\n\nETTm2\n\n ETTm2 (freq:str='15T', name:str='ETTm2', n_ts:int=7, n_time:int=57600,\n        test_size:int=11520, val_size:int=11520, horizons:Tuple[int]=(96,\n        192, 336, 720))\n\nThe ETTm2 dataset monitors an electricity transformer from a region of a province of China including oil temperature and variants of load (such as high useful load and high useless load) from July 2016 to July 2018 at a fifteen minute frequency.\nReference: Zhou, et al. Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting. AAAI 2021. https://arxiv.org/abs/2012.07436\n\nsource\n\n\nETTm1\n\n ETTm1 (freq:str='15T', name:str='ETTm1', n_ts:int=7, n_time:int=57600,\n        test_size:int=11520, val_size:int=11520, horizons:Tuple[int]=(96,\n        192, 336, 720))\n\nThe ETTm1 dataset monitors an electricity transformer from a region of a province of China including oil temperature and variants of load (such as high useful load and high useless load) from July 2016 to July 2018 at a fifteen minute frequency.\n\nsource\n\n\nETTh2\n\n ETTh2 (freq:str='H', name:str='ETTh2', n_ts:int=7, n_time:int=14400,\n        test_size:int=2880, val_size:int=2880, horizons:Tuple[int]=(96,\n        192, 336, 720))\n\nThe ETTh2 dataset monitors an electricity transformer from a region of a province of China including oil temperature and variants of load (such as high useful load and high useless load) from July 2016 to July 2018 at an hourly frequency.\n\nsource\n\n\nETTh1\n\n ETTh1 (freq:str='H', name:str='ETTh1', n_ts:int=7, n_time:int=14400,\n        test_size:int=2880, val_size:int=2880, horizons:Tuple[int]=(96,\n        192, 336, 720))\n\nThe ETTh1 dataset monitors an electricity transformer from a region of a province of China including oil temperature and variants of load (such as high useful load and high useless load) from July 2016 to July 2018 at an hourly frequency.\n\nsource\n\n\nLongHorizon2\n\n LongHorizon2 (source_url:str='https://www.dropbox.com/s/rlc1qmprpvuqrsv/a\n               ll_six_datasets.zip?dl=1')\n\nThis Long-Horizon datasets wrapper class, provides with utility to download and wrangle the following datasets:\nETT, ECL, Exchange, Traffic, ILI and Weather.\n\nEach set is normalized with the train data mean and standard deviation.\nDatasets are partitioned into train, validation and test splits.\nFor all datasets: 70%, 10%, and 20% of observations are train, validation, test, except ETT that uses 20% validation."
  },
  {
    "objectID": "m4.html",
    "href": "m4.html",
    "title": "M4 dataset",
    "section": "",
    "text": "M4 meta information\nsource"
  },
  {
    "objectID": "m4.html#download-data-class",
    "href": "m4.html#download-data-class",
    "title": "M4 dataset",
    "section": "Download data class",
    "text": "Download data class\n\nsource\n\nM4\n\n M4 (source_url:str='https://raw.githubusercontent.com/Mcompetitions/M4-me\n     thods/master/Dataset/', naive2_forecast_url:str='https://github.com/N\n     ixtla/m4-forecasts/raw/master/forecasts/submission-Naive2.zip')\n\n\ngroup = 'Hourly'\nawait M4.async_download('data', group=group)\ndf, *_ = M4.load(directory='data', group=group)\nn_series = len(np.unique(df.unique_id.values))\ndisplay_str  = f'Group: {group} '\ndisplay_str += f'n_series: {n_series}'\nprint(display_str)"
  },
  {
    "objectID": "m4.html#evaluation-class",
    "href": "m4.html#evaluation-class",
    "title": "M4 dataset",
    "section": "Evaluation class",
    "text": "Evaluation class\n\nsource\n\nM4Evaluation\n\n M4Evaluation ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nURL-based evaluation\nThe method evaluate from the class M4Evaluation can receive a url of a benchmark uploaded to the M4 competiton.\nThe results compared to the on-the-fly evaluation were obtained from the official evaluation.\n\nfrom fastcore.test import test_close\n\n\nesrnn_url = 'https://github.com/Nixtla/m4-forecasts/raw/master/forecasts/submission-118.zip'\nesrnn_evaluation = M4Evaluation.evaluate('data', 'Hourly', esrnn_url)\n# Test of the same evaluation as the original one\ntest_close(esrnn_evaluation['SMAPE'].item(), 9.328, eps=1e-3)\ntest_close(esrnn_evaluation['MASE'].item(), 0.893, eps=1e-3)\ntest_close(esrnn_evaluation['OWA'].item(), 0.440, eps=1e-3)\nesrnn_evaluation\n\n\n\nNumpy-based evaluation\nAlso the method evaluate can recevie a numpy array of forecasts.\n\nfforma_url = 'https://github.com/Nixtla/m4-forecasts/raw/master/forecasts/submission-245.zip'\nfforma_forecasts = M4Evaluation.load_benchmark('data', 'Hourly', fforma_url)\nfforma_evaluation = M4Evaluation.evaluate('data', 'Hourly', fforma_forecasts)\n# Test of the same evaluation as the original one\ntest_close(fforma_evaluation['SMAPE'].item(), 11.506, eps=1e-3)\ntest_close(fforma_evaluation['MASE'].item(), 0.819, eps=1e-3)\ntest_close(fforma_evaluation['OWA'].item(), 0.484, eps=1e-3)\nfforma_evaluation"
  },
  {
    "objectID": "favorita.html",
    "href": "favorita.html",
    "title": "Favorita",
    "section": "",
    "text": "This auxiliary functions are used to efficiently create and wrangle Favorita’s series.\n\n\n\nsource\n\n\n\n numpy_balance (*arrs)\n\nFast NumPy implementation of ‘balance’ operation, useful to create a balanced panel dataset, ie a dataset with all the interactions of ‘unique_id’ and ‘ds’.\nParameters: arrs: NumPy arrays.\nReturns: out: NumPy array.\n\nsource\n\n\n\n\n numpy_ffill (arr)\n\nFast NumPy implementation of ffill that fills missing values in an array by propagating the last non-missing value forward.\nFor example, if the array has the following values: 0 1 2 3 1 2 NaN 4\nThe ffill method would fill the missing values as follows: 0 1 2 3 1 2 2 4\nParameters: arr: NumPy array.\nReturns: out: NumPy array.\n\nsource\n\n\n\n\n numpy_bfill (arr)\n\nFast NumPy implementation of bfill that fills missing values in an array by propagating the last non-missing value backwards.\nFor example, if the array has the following values: 0 1 2 3 1 2 NaN 4\nThe bfill method would fill the missing values as follows: 0 1 2 3 1 2 4 4\nParameters: arr: NumPy array.\nReturns: out: NumPy array.\n\n\n\n\n\nsource\n\n\n\n one_hot_encoding (df, index_col)\n\nEncodes dataFrame df’s categorical variables skipping index_col.\nParameters: df: pd.DataFrame with categorical columns. index_col: str, the index column to avoid encoding.\nReturns: one_hot_concat_df: pd.DataFrame with one hot encoded categorical columns.\n\nsource\n\n\n\n\n nested_one_hot_encoding (df, index_col)\n\nEncodes dataFrame df’s hierarchically-nested categorical variables skipping index_col.\nNested categorical variables (example geographic levels country&gt;state), require the dummy features to preserve encoding order, to reflect the hierarchy of the categorical variables.\nParameters: df: pd.DataFrame with hierarchically-nested categorical columns. index_col: str, the index column to avoid encoding.\nReturns: one_hot_concat_df: pd.DataFrame with one hot encoded hierarchically-nested categorical columns.\n\nsource\n\n\n\n\n get_levels_from_S_df (S_df)\n\nGet hierarchical index levels implied by aggregation constraints dataframe S_df.\nCreate levels from summation matrix (base, bottom). Goes through the rows until all the bottom level series are ‘covered’ by the aggregation constraints to discover blocks/hierarchy levels.\nParameters: S_df: pd.DataFrame with summing matrix of size (base, bottom), see aggregate method.\nReturns: levels: list, with hierarchical aggregation indexes, where each entry is a level."
  },
  {
    "objectID": "favorita.html#auxiliary-functions",
    "href": "favorita.html#auxiliary-functions",
    "title": "Favorita",
    "section": "",
    "text": "This auxiliary functions are used to efficiently create and wrangle Favorita’s series.\n\n\n\nsource\n\n\n\n numpy_balance (*arrs)\n\nFast NumPy implementation of ‘balance’ operation, useful to create a balanced panel dataset, ie a dataset with all the interactions of ‘unique_id’ and ‘ds’.\nParameters: arrs: NumPy arrays.\nReturns: out: NumPy array.\n\nsource\n\n\n\n\n numpy_ffill (arr)\n\nFast NumPy implementation of ffill that fills missing values in an array by propagating the last non-missing value forward.\nFor example, if the array has the following values: 0 1 2 3 1 2 NaN 4\nThe ffill method would fill the missing values as follows: 0 1 2 3 1 2 2 4\nParameters: arr: NumPy array.\nReturns: out: NumPy array.\n\nsource\n\n\n\n\n numpy_bfill (arr)\n\nFast NumPy implementation of bfill that fills missing values in an array by propagating the last non-missing value backwards.\nFor example, if the array has the following values: 0 1 2 3 1 2 NaN 4\nThe bfill method would fill the missing values as follows: 0 1 2 3 1 2 4 4\nParameters: arr: NumPy array.\nReturns: out: NumPy array.\n\n\n\n\n\nsource\n\n\n\n one_hot_encoding (df, index_col)\n\nEncodes dataFrame df’s categorical variables skipping index_col.\nParameters: df: pd.DataFrame with categorical columns. index_col: str, the index column to avoid encoding.\nReturns: one_hot_concat_df: pd.DataFrame with one hot encoded categorical columns.\n\nsource\n\n\n\n\n nested_one_hot_encoding (df, index_col)\n\nEncodes dataFrame df’s hierarchically-nested categorical variables skipping index_col.\nNested categorical variables (example geographic levels country&gt;state), require the dummy features to preserve encoding order, to reflect the hierarchy of the categorical variables.\nParameters: df: pd.DataFrame with hierarchically-nested categorical columns. index_col: str, the index column to avoid encoding.\nReturns: one_hot_concat_df: pd.DataFrame with one hot encoded hierarchically-nested categorical columns.\n\nsource\n\n\n\n\n get_levels_from_S_df (S_df)\n\nGet hierarchical index levels implied by aggregation constraints dataframe S_df.\nCreate levels from summation matrix (base, bottom). Goes through the rows until all the bottom level series are ‘covered’ by the aggregation constraints to discover blocks/hierarchy levels.\nParameters: S_df: pd.DataFrame with summing matrix of size (base, bottom), see aggregate method.\nReturns: levels: list, with hierarchical aggregation indexes, where each entry is a level."
  },
  {
    "objectID": "favorita.html#favorita-dataset",
    "href": "favorita.html#favorita-dataset",
    "title": "Favorita",
    "section": "Favorita Dataset",
    "text": "Favorita Dataset\n\nFavorita Raw\n\nsource\n\nFavoritaRawData\n\n FavoritaRawData ()\n\nFavorita Raw Data\nRaw subset datasets from the Favorita 2018 Kaggle competition. This class contains utilities to download, load and filter portions of the dataset.\nIf you prefer, you can also download original dataset available from Kaggle directly. pip install kaggle --upgrade kaggle competitions download -c favorita-grocery-sales-forecasting\n\nsource\n\n\nFavoritaRawData._load_raw_group_data\n\n FavoritaRawData._load_raw_group_data (directory, group, verbose=False)\n\nLoad raw group data.\nReads, filters and sorts Favorita subset dataset.\nParameters: directory: str, Directory where data will be downloaded. group: str, dataset group name in ‘Favorita200’, ‘Favorita500’, ‘FavoritaComplete’. verbose: bool=False, wether or not print partial outputs.\nReturns: filter_items: ordered list with unique items identifiers in the Favorita subset. filter_stores: ordered list with unique store identifiers in the Favorita subset. filter_dates: ordered list with dates in the Favorita subset. raw_group_data: dictionary with original raw Favorita pd.DataFrames, temporal, oil, items, store_info, holidays, transactions. \n\n\nFavorita Raw Usage example\n\nfrom datasetsforecast.favorita import FavoritaRawData\n\nverbose = True\ngroup = 'Favorita200' # 'Favorita500', 'FavoritaComplete'\ndirectory = './data/favorita' # directory = f's3://favorita'\n\nfilter_items, filter_stores, filter_dates, raw_group_data = \\\n    FavoritaRawData._load_raw_group_data(directory=directory, group=group, verbose=verbose)\nn_items  = len(filter_items)\nn_stores = len(filter_stores)\nn_dates  = len(filter_dates)\n\nprint('\\n')\nprint('n_stores: \\t', n_stores)\nprint('n_items: \\t', n_items)\nprint('n_dates: \\t', n_dates)\nprint('n_items * n_dates: \\t\\t',n_items * n_dates)\nprint('n_items * n_stores: \\t\\t',n_items * n_stores)\nprint('n_items * n_dates * n_stores: \\t', n_items * n_dates * n_stores)\n\n\n\n\nFavoritaData\n\nsource\n\nFavoritaData\n\n FavoritaData ()\n\nFavorita Data\nThe processed Favorita dataset of grocery contains item sales daily history with additional information on promotions, items, stores, and holidays, containing 371,312 series from January 2013 to August 2017, with a geographic hierarchy of states, cities, and stores. This wrangling matches that of the DPMN paper.\n\nKin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker (2022).”Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures”. International Journal Forecasting, special issue.\n\n\nsource\n\n\nFavoritaData.load_preprocessed\n\n FavoritaData.load_preprocessed (directory:str, group:str,\n                                 cache:bool=True, verbose:bool=False)\n\nLoad Favorita group datasets.\nFor the exploration of more complex models, we make available the entire information including data at the bottom level of the items sold in Favorita stores, in addition to the aggregate/national level information for the items.\nParameters: directory: str, directory where data will be downloaded and saved. group: str, dataset group name in ‘Favorita200’, ‘Favorita500’, ‘FavoritaComplete’. cache: bool=False, If True saves and loads. verbose: bool=False, wether or not print partial outputs.\nReturns: static_bottom: pd.DataFrame, with static variables of bottom level series. static_agg: pd.DataFrame, with static variables of aggregate level series. temporal_bottom: pd.DataFrame, with temporal variables of bottom level series. temporal_agg: pd.DataFrame, with temporal variables of aggregate level series.\n\nsource\n\n\nFavoritaData.load\n\n FavoritaData.load (directory:str, group:str, cache:bool=True,\n                    verbose:bool=False)\n\nLoad Favorita forecasting benchmark dataset.\nIn contrast with other hierarchical datasets, this dataset contains a geographic hierarchy for each individual grocery item series, identified with ‘item_id’ column. The geographic hierarchy is captured by the ‘hier_id’ column.\nFor this reason minor wrangling is needed to adapt it for use with HierarchicalForecast, and StatsForecast libraries.\nParameters: directory: str, directory where data will be downloaded and saved. group: str, dataset group name in ‘Favorita200’, ‘Favorita500’, ‘FavoritaComplete’. cache: bool=False, If True saves and loads. verbose: bool=False, wether or not print partial outputs.\nReturns: Y_df: pd.DataFrame, target base time series with columns [‘item_id’, ‘hier_id’, ‘ds’, ‘y’]. S_df: pd.DataFrame, hierarchical constraints dataframe of size (base, bottom).\n\n# #| hide\n# #| eval: false\n# # Test the equality of created and loaded datasets columns and rows\n# static_agg1, static_bottom1, temporal_agg1, temporal_bottom1, S_df1 = \\\n#                         FavoritaData.load_preprocessed(directory=directory, group=group, cache=False)\n\n# static_agg2, static_bottom2, temporal_agg2, temporal_bottom2, S_df2 = \\\n#                         FavoritaData.load_preprocessed(directory=directory, group=group)\n\n# test_eq(len(static_agg1)+len(static_agg1.columns), \n#         len(static_agg2)+len(static_agg2.columns))\n# test_eq(len(static_bottom1)+len(static_bottom1.columns), \n#         len(static_bottom2)+len(static_bottom2.columns))\n\n# test_eq(len(temporal_agg1)+len(temporal_agg1.columns), \n#         len(temporal_agg2)+len(temporal_agg2.columns))\n# test_eq(len(temporal_bottom1)+len(temporal_bottom1.columns), \n#         len(temporal_bottom2)+len(temporal_bottom2.columns))\n\n\n\nFavorita Usage Example\n\n# Qualitative evaluation of hierarchical data\nfrom datasetsforecast.favorita import FavoritaData\nfrom hierarchicalforecast.utils import HierarchicalPlot\n\ngroup = 'Favorita200' # 'Favorita500', 'FavoritaComplete'\ndirectory = './data/favorita'\nY_df, S_df, tags = FavoritaData.load(directory=directory, group=group)\n\nY_item_df = Y_df[Y_df.item_id==1916577] # 112830, 1501570, 1916577\nY_item_df = Y_item_df.rename(columns={'hier_id': 'unique_id'})\nY_item_df = Y_item_df.set_index('unique_id')\ndel Y_item_df['item_id']\n\nhplots = HierarchicalPlot(S=S_df, tags=tags)\nhplots.plot_hierarchically_linked_series(\n    Y_df=Y_item_df, bottom_series='store_[40]',\n)"
  },
  {
    "objectID": "phm2008.html",
    "href": "phm2008.html",
    "title": "PHM2008 dataset",
    "section": "",
    "text": "source\n\nFD004\n\n FD004 (seasonality:int=1, horizon:int=8, freq:str='None',\n        train_file:str='train_FD004.txt', test_file:str='test_FD004.txt',\n        rul_file:str='RUL_FD004.txt', n_ts:int=249, n_test:int=248)\n\n\nsource\n\n\nFD003\n\n FD003 (seasonality:int=1, horizon:int=1, freq:str='None',\n        train_file:str='train_FD003.txt', test_file:str='test_FD003.txt',\n        rul_file:str='RUL_FD003.txt', n_ts:int=100, n_test:int=100)\n\n\nsource\n\n\nFD002\n\n FD002 (seasonality:int=1, horizon:int=1, freq:str='None',\n        train_file:str='train_FD002.txt', test_file:str='test_FD002.txt',\n        rul_file:str='RUL_FD002.txt', n_ts:int=260, n_test:int=259)\n\n\nsource\n\n\nFD001\n\n FD001 (seasonality:int=1, horizon:int=1, freq:str='None',\n        train_file:str='train_FD001.txt', test_file:str='test_FD001.txt',\n        rul_file:str='RUL_FD001.txt', n_ts:int=100, n_test:int=100)\n\n\nsource\n\n\nPHM2008\n\n PHM2008 ()"
  }
]