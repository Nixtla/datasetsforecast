{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pickle\n",
    "from functools import partial\n",
    "from inspect import signature\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fugue import transform\n",
    "from fugue.collections.yielded import Yielded\n",
    "from fugue.constants import FUGUE_CONF_WORKFLOW_EXCEPTION_INJECT\n",
    "from fugue.dataframe import DataFrame\n",
    "from fugue.workflow import FugueWorkflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _cotransform(\n",
    "    df1: Any,\n",
    "    df2: Any,\n",
    "    using: Any,\n",
    "    schema: Any = None,\n",
    "    params: Any = None,\n",
    "    partition: Any = None,\n",
    "    engine: Any = None,\n",
    "    engine_conf: Any = None,\n",
    "    force_output_fugue_dataframe: bool = False,\n",
    "    as_local: bool = False,\n",
    ") -> Any:\n",
    "    dag = FugueWorkflow(compile_conf={FUGUE_CONF_WORKFLOW_EXCEPTION_INJECT: 0})\n",
    "    src = dag.create_data(df1).zip(dag.create_data(df2), partition=partition)\n",
    "    tdf = src.transform(\n",
    "        using=using,\n",
    "        schema=schema,\n",
    "        params=params,\n",
    "        pre_partition=partition,\n",
    "    )\n",
    "    tdf.yield_dataframe_as(\"result\", as_local=as_local)\n",
    "    dag.run(engine, conf=engine_conf)\n",
    "    result = dag.yields[\"result\"].result  # type:ignore\n",
    "    if force_output_fugue_dataframe or isinstance(df1, (DataFrame, Yielded)):\n",
    "        return result\n",
    "    return result.as_pandas() if result.is_local else result.native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _quantiles_from_levels(model_name, level):\n",
    "    \"\"\"Returns quantiles associated to `level` and the sorte columns of `model_name`\"\"\"\n",
    "    level = sorted(level)\n",
    "    alphas = [100 - lv for lv in level]\n",
    "    quantiles = [alpha / 200 for alpha in reversed(alphas)]\n",
    "    quantiles.extend([1 - alpha / 200 for alpha in alphas])\n",
    "    cols = [f'{model_name}-lo-{lv}' for lv in reversed(level)]\n",
    "    cols.extend([f'{model_name}-hi-{lv}' for lv in level])\n",
    "    return np.array(quantiles), cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq, test_fail, test_close, test_warns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "levels = range(101)\n",
    "quantiles, cols = _quantiles_from_levels('mymodel', list(levels))\n",
    "test_close(\n",
    "    np.linspace(0., 1., num=len(quantiles)),\n",
    "    quantiles, eps=1e-2\n",
    ")\n",
    "test_eq(\n",
    "    cols,\n",
    "    [f'mymodel-lo-{lv}' for lv in reversed(levels)] + [f'mymodel-hi-{lv}' for lv in levels]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _evaluate(\n",
    "        df: pd.DataFrame, \n",
    "        df_train: pd.DataFrame,\n",
    "        metrics: Optional[List[Callable]] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        level: Optional[List] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "    cols_to_rm = '|'.join([id_col, time_col, target_col, 'cutoff', 'lo', 'hi'])\n",
    "    has_cutoff = 'cutoff' in df.columns\n",
    "    models = df.loc[:, ~df.columns.str.contains(cols_to_rm)].columns\n",
    "    y = df[target_col].values\n",
    "    eval_ = {}\n",
    "    for model in models:\n",
    "        eval_[model] = {}\n",
    "        for metric in metrics:\n",
    "            y_hat = df[model].values\n",
    "            metric_name = metric.__name__\n",
    "            params = signature(metric).parameters\n",
    "            if 'y_train' in params:\n",
    "                if df_train is None:\n",
    "                    raise Exception(f'Please provide `Y_df` to compute {metric_name}')\n",
    "                eval_[model][metric_name] = metric(y, y_hat, y_train=df_train[target_col].values)\n",
    "            elif 'quantiles' in params:\n",
    "                if level is None:\n",
    "                    raise Exception(\n",
    "                        f'Please provide the `level` argument to compute {metric_name}. '\n",
    "                    )\n",
    "                quantiles, lv_cols = _quantiles_from_levels(model_name=model, level=level)\n",
    "                y_hat_q = df[lv_cols].values\n",
    "                eval_[model][metric_name] = metric(y, y_hat_q, quantiles=quantiles)\n",
    "            elif ('y_hat_lo' in params) and ('y_hat_hi' in params):\n",
    "                if level is None:\n",
    "                    raise Exception(\n",
    "                        f'Please provide the `level` argument to compute {metric_name}. '\n",
    "                    )\n",
    "                for lv in level:\n",
    "                    y_hat_lo = df[f'{model}-lo-{lv}'].values\n",
    "                    y_hat_hi = df[f'{model}-hi-{lv}'].values\n",
    "                    eval_[model][f'{metric_name}-lv-{lv}'] = metric(y, y_hat_lo, y_hat_hi)\n",
    "            else:\n",
    "                eval_[model][metric_name] = metric(y, y_hat)\n",
    "    eval_df = pd.DataFrame(eval_).rename_axis('metric').reset_index()\n",
    "    if has_cutoff:\n",
    "        eval_df.insert(0, 'cutoff', df['cutoff'].iloc[0])\n",
    "    eval_df.insert(0, id_col, df[id_col].iloc[0])\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _evaluate_without_insample(\n",
    "        df: pd.DataFrame, \n",
    "        metrics: Optional[List[Callable]] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        level: Optional[List] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "    return _evaluate(\n",
    "        df=df, df_train=None, metrics=metrics, id_col=id_col, time_col=time_col,\n",
    "        target_col=target_col, level=level,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _schema_evaluate(\n",
    "        df: pd.DataFrame,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "    ) -> str: \n",
    "    cols_to_rm = '|'.join([id_col, time_col, target_col, 'cutoff', 'lo', 'hi'])\n",
    "    has_cutoff = 'cutoff' in df.columns\n",
    "    models = df.loc[:, ~df.columns.str.contains(cols_to_rm)].columns\n",
    "    str_models = ','.join([f\"{model}:double\" for model in models])\n",
    "    dtypes = df.dtypes\n",
    "    id_col_type = dtypes.loc[id_col]\n",
    "    if id_col_type == 'category':\n",
    "        raise NotImplementedError(\n",
    "            'Use of `category` type to identify each time series is not yet implemented. '\n",
    "            f'Please transform your {id_col} to string to continue.'\n",
    "        )\n",
    "    id_col_type = 'string' if id_col_type == 'object' else id_col_type\n",
    "    cutoff_col_type = ''\n",
    "    if has_cutoff:\n",
    "        cutoff_col_type = f\"{dtypes.loc['cutoff']}\".replace('64[ns]', '')\n",
    "        cutoff_col_type = 'string' if cutoff_col_type == 'object' else cutoff_col_type\n",
    "    schema = (\n",
    "        f'{id_col}:{id_col_type},metric:string,'\n",
    "        + (f'cutoff:{cutoff_col_type},' if has_cutoff else '')\n",
    "        + str_models\n",
    "    )\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _agg_evaluation(\n",
    "        df_eval: pd.DataFrame, \n",
    "        agg_fn: Any, \n",
    "        agg_by: Any,\n",
    "    ) -> pd.DataFrame:\n",
    "    cols_to_rm = '|'.join(agg_by + ['unique_id', 'metric', 'cutoff', 'lo', 'hi'])\n",
    "    models = df_eval.loc[:, ~df_eval.columns.str.contains(cols_to_rm)].columns\n",
    "    return df_eval.groupby(agg_by)[models].apply(agg_fn, axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _schema_agg_evaluation(\n",
    "        df: pd.DataFrame, \n",
    "        agg_by: Optional[List[str]] = None,\n",
    "    ) -> str:\n",
    "    cols_to_rm = '|'.join(agg_by + ['unique_id', 'metric', 'cutoff', 'lo', 'hi'])\n",
    "    models = df.loc[:, ~df.columns.str.contains(cols_to_rm)].columns\n",
    "    str_models = ','.join([f'{model}:double' for model in models])\n",
    "    dtypes = df.dtypes\n",
    "    agg_by_types = [dtypes.loc[col] for col in agg_by]\n",
    "    if 'category' in agg_by_types:\n",
    "        raise NotImplementedError(\n",
    "            'Use of `category` type is not yet implemented. '\n",
    "            f'Please transform your columns to string to continue.'\n",
    "        )\n",
    "    agg_by_types = ['string' if col == 'object' else col for col in agg_by_types]\n",
    "    agg_by_types = [col.replace('64[ns]', '') for col in agg_by_types]\n",
    "    schema = [f'{col}:{type_}' for col, type_ in zip(agg_by, agg_by_types)]\n",
    "    schema = ','.join(schema) + ',' + str_models\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from datasetsforecast.m4 import M4, M4Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/1l3vkh3x4_q3s4r36b60s8f00000gn/T/ipykernel_13221/3530693183.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_hourly['FFORMA'] = eval_test['FFORMA'].values\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "fforma_url = 'https://github.com/Nixtla/m4-forecasts/raw/master/forecasts/submission-245.zip'\n",
    "M4Evaluation.load_benchmark('data', 'Hourly', fforma_url)\n",
    "eval_test = pd.read_csv('data/m4/datasets/submission-245.csv').query(\"id.str.startswith('H')\")\n",
    "eval_test = eval_test.set_index('id')\n",
    "eval_test.columns = list(range(1, 49))\n",
    "eval_test = eval_test.unstack().reset_index()\n",
    "eval_test.columns = ['ds', 'unique_id', 'FFORMA']\n",
    "eval_test = eval_test[['unique_id', 'ds', 'FFORMA']].sort_values(['unique_id', 'ds'])\n",
    "hourly, *_ = M4.load('data', 'Hourly')\n",
    "test_hourly = hourly.groupby('unique_id').tail(48)\n",
    "train_hourly = hourly.drop(test_hourly.index)\n",
    "test_hourly['FFORMA'] = eval_test['FFORMA'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(\n",
    "    _schema_evaluate(eval_test),\n",
    "    'unique_id:string,metric:string,FFORMA:double'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def accuracy(\n",
    "        Y_hat_df: pd.DataFrame,\n",
    "        metrics: List[Callable],\n",
    "        Y_test_df: Optional[pd.DataFrame] = None,\n",
    "        Y_df: Optional[pd.DataFrame] = None,\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        level: Optional[List] = None,\n",
    "        agg_by: Optional[List[str]] = None,\n",
    "        agg_fn: Callable = np.mean,\n",
    "        engine: Any = None,\n",
    "        **transform_kwargs: Any,\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"Evaluate forecast using different metrics.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_hat_df : pandas DataFrame\n",
    "        Forecasts and models to evaluate.\n",
    "        Can contain the actual values given by `target_col`.\n",
    "    metrics : List of Callables\n",
    "        Functions with arguments `y`, `y_hat`, and optionally `y_train`.\n",
    "    Y_test_df :  pandas DataFrame, optional (default=None)\n",
    "        True values. \n",
    "        Nedded if `Y_hat_df` does not have the true values.\n",
    "    Y_df : pandas DataFrame, optional (default=None)\n",
    "        Training set. Used to evaluate metrics such as `mase`. \n",
    "    id_col : str (default='unique_id')\n",
    "        Column that identifies each serie. If 'index' then the index is used.\n",
    "    time_col : str (default='ds')\n",
    "        Column that identifies each timestep, its values can be timestamps or integers.\n",
    "    target_col : str (default='y')\n",
    "        Column that contains the target.\n",
    "    agg_by: List[str], optional (default=None)\n",
    "        List of columns to aggregate the results.\n",
    "        To get metrics per time series use [`id_col`].\n",
    "    agg_fn: Callable, (default=np.mean)\n",
    "        Function to aggregate metrics.\n",
    "    engine: Any\n",
    "        Engine to distributed computing.\n",
    "    transform_kwargs: Any\n",
    "        Extra arguments to pass to fugue's `transform`.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    result : pandas DataFrame\n",
    "        Metrics with one column per model.\n",
    "    \"\"\"\n",
    "    if 'y' not in Y_hat_df.columns:\n",
    "        raise Exception(\n",
    "            'Please include the actual values in `Y_hat_df` '\n",
    "            'or pass `Y_test_df`.'\n",
    "        )\n",
    "    df = Y_hat_df if Y_test_df is None else Y_hat_df.merge(Y_test_df, how='left', on=[id_col, time_col])\n",
    "    transform_fn = partial(_cotransform, df2=Y_df) if Y_df is not None else transform   \n",
    "    if Y_df is None:\n",
    "        fn = _evaluate_without_insample\n",
    "    else:\n",
    "        fn = _evaluate\n",
    "    has_cutoff = 'cutoff' in Y_hat_df.columns\n",
    "    evaluation_df = transform_fn(\n",
    "        df, \n",
    "        using=fn, \n",
    "        engine=engine, \n",
    "        params=dict(\n",
    "            metrics=metrics,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            level=level,\n",
    "        ), \n",
    "        schema=_schema_evaluate(\n",
    "            df, \n",
    "            id_col=id_col, \n",
    "            time_col=time_col, \n",
    "            target_col=target_col,\n",
    "        ), \n",
    "        partition=dict(by=id_col) if not has_cutoff else dict(by=[id_col, 'cutoff']),\n",
    "    )\n",
    "    if agg_by is not None:\n",
    "        agg_by = ['metric'] + agg_by\n",
    "    else:\n",
    "        agg_by = ['metric']\n",
    "    evaluation_df = transform(\n",
    "        evaluation_df,\n",
    "        using=_agg_evaluation,\n",
    "        engine=engine,\n",
    "        params=dict(agg_fn=agg_fn, agg_by=agg_by),\n",
    "        schema=_schema_agg_evaluation(evaluation_df, agg_by),\n",
    "        partition=agg_by,\n",
    "        **transform_kwargs,\n",
    "    )\n",
    "    return evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from datasetsforecast.losses import smape, mase, mape, mqloss, mae, scaled_crps, coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def mase_24(y, y_hat, y_train):\n",
    "    return mase(y, y_hat, y_train, seasonality=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_cutoff = pd.concat([test_hourly.assign(cutoff=str(i), model1=0) for i in range(3)])\n",
    "eval_cutoff = accuracy(test_cutoff, metrics=[smape, mape], agg_by=['cutoff'])\n",
    "test_eq(\n",
    "    eval_cutoff.shape,\n",
    "    (6, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_close(\n",
    "    accuracy(test_hourly, Y_df=train_hourly, metrics=[smape, mase_24])['FFORMA'].values,\n",
    "    np.array([0.818598, 11.505702])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(\n",
    "    accuracy(test_hourly, Y_df=train_hourly, metrics=[smape]),\n",
    "    accuracy(test_hourly, metrics=[smape])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:_1 _State.RUNNING -> _State.FAILED  Please provide `Y_df` to compute mase\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "test_fail(\n",
    "    lambda : accuracy(test_hourly, metrics=[smape, mase]), \n",
    "    contains='mase'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:_1 _State.RUNNING -> _State.FAILED  Please provide the `level` argument to compute mqloss. \n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "test_fail(\n",
    "    lambda : accuracy(test_hourly, metrics=[smape, mqloss]), \n",
    "    contains='mqloss'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/1l3vkh3x4_q3s4r36b60s8f00000gn/T/ipykernel_13221/664392555.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_hourly_series[f'FFORMA-lo-{lv}'] = test_hourly_series['y'] - alpha\n",
      "/var/folders/7x/1l3vkh3x4_q3s4r36b60s8f00000gn/T/ipykernel_13221/664392555.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_hourly_series[f'FFORMA-hi-{lv}'] = test_hourly_series['y'] + alpha\n",
      "/var/folders/7x/1l3vkh3x4_q3s4r36b60s8f00000gn/T/ipykernel_13221/664392555.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_hourly_series[f'FFORMA-lo-{lv}'] = test_hourly_series['y'] - alpha\n",
      "/var/folders/7x/1l3vkh3x4_q3s4r36b60s8f00000gn/T/ipykernel_13221/664392555.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_hourly_series[f'FFORMA-hi-{lv}'] = test_hourly_series['y'] + alpha\n",
      "/var/folders/7x/1l3vkh3x4_q3s4r36b60s8f00000gn/T/ipykernel_13221/664392555.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_hourly_series[f'FFORMA-lo-{lv}'] = test_hourly_series['y'] - alpha\n",
      "/var/folders/7x/1l3vkh3x4_q3s4r36b60s8f00000gn/T/ipykernel_13221/664392555.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_hourly_series[f'FFORMA-hi-{lv}'] = test_hourly_series['y'] + alpha\n",
      "/var/folders/7x/1l3vkh3x4_q3s4r36b60s8f00000gn/T/ipykernel_13221/664392555.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_hourly_series[f'FFORMA-lo-{lv}'] = test_hourly_series['y'] - alpha\n",
      "/var/folders/7x/1l3vkh3x4_q3s4r36b60s8f00000gn/T/ipykernel_13221/664392555.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_hourly_series[f'FFORMA-hi-{lv}'] = test_hourly_series['y'] + alpha\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# add level forecasting\n",
    "test_hourly_series = test_hourly.query('unique_id == \"H1\"')\n",
    "level = [50, 80, 90, 99]\n",
    "for lv in level:\n",
    "    alpha = (100 - lv) / 200\n",
    "    test_hourly_series[f'FFORMA-lo-{lv}'] = test_hourly_series['y'] - alpha\n",
    "    test_hourly_series[f'FFORMA-hi-{lv}'] = test_hourly_series['y'] + alpha\n",
    "evaluation = accuracy(\n",
    "    test_hourly_series, \n",
    "    metrics=[smape, mqloss, mae, scaled_crps, coverage], level=level\n",
    ")\n",
    "test_close(\n",
    "    evaluation.query('metric == \"mqloss\"')['FFORMA'].iloc[0],\n",
    "    np.mean([((100 - lv)/ 200) ** 2 for lv in level])\n",
    ")\n",
    "test_close(\n",
    "    evaluation.query('metric == \"scaled_crps\"')['FFORMA'].iloc[0],\n",
    "    np.mean([2 * (((100 - lv)/ 200) ** 2) * len(test_hourly_series)/test_hourly_series['y'].sum() for lv in level])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test coverage\n",
    "y = np.random.normal(size=1_000)\n",
    "test_df = pd.DataFrame({\n",
    "    'unique_id': '1', \n",
    "    'ds': np.arange(1_000).astype(int), \n",
    "    'model1': y,\n",
    "    'y': y,\n",
    "})\n",
    "level = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "for lv in level:\n",
    "    alpha_lo = (100 - lv) / 200\n",
    "    alpha_hi = 1 - alpha_lo\n",
    "    test_df[f'model1-lo-{lv}'] = np.quantile(test_df['y'], q=alpha_lo)\n",
    "    test_df[f'model1-hi-{lv}'] = np.quantile(test_df['y'], q=alpha_hi)\n",
    "evaluation = accuracy(\n",
    "    test_df, \n",
    "    metrics=[mqloss, scaled_crps, coverage], level=level\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>model1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coverage-lv-0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coverage-lv-10</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coverage-lv-100</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coverage-lv-20</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coverage-lv-30</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>coverage-lv-40</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>coverage-lv-50</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>coverage-lv-60</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>coverage-lv-70</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>coverage-lv-80</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>coverage-lv-90</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mqloss</td>\n",
       "      <td>0.276908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>scaled_crps</td>\n",
       "      <td>0.688474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             metric    model1\n",
       "0     coverage-lv-0  0.000000\n",
       "1    coverage-lv-10  0.100000\n",
       "2   coverage-lv-100  1.000000\n",
       "3    coverage-lv-20  0.200000\n",
       "4    coverage-lv-30  0.300000\n",
       "5    coverage-lv-40  0.400000\n",
       "6    coverage-lv-50  0.500000\n",
       "7    coverage-lv-60  0.600000\n",
       "8    coverage-lv-70  0.700000\n",
       "9    coverage-lv-80  0.800000\n",
       "10   coverage-lv-90  0.900000\n",
       "11           mqloss  0.276908\n",
       "12      scaled_crps  0.688474"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
