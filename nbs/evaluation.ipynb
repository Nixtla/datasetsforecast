{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Any, Callable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fugue import transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _evaluate(\n",
    "        df: pd.DataFrame, \n",
    "        metrics: List[Callable],\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        level: Optional[List] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "    eval_ = {}\n",
    "    cols_to_rm = '|'.join([id_col, time_col, target_col, 'cutoff', 'lo', 'hi'])\n",
    "    has_cutoff = 'cutoff' in df.columns\n",
    "    models = df.loc[:, ~df.columns.str.contains(cols_to_rm)].columns\n",
    "    for model in models:\n",
    "        eval_[model] = {}\n",
    "        for metric in metrics:\n",
    "            eval_[model][metric.__name__] = metric(df[target_col], df[model])\n",
    "    eval_df = pd.DataFrame(eval_).rename_axis('metric').reset_index()\n",
    "    if has_cutoff:\n",
    "        eval_df.insert(0, 'cutoff', df['cutoff'].iloc[0])\n",
    "    eval_df.insert(0, id_col, df[id_col].iloc[0])\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _schema_evaluation(\n",
    "        df: pd.DataFrame, \n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "    ):\n",
    "    cols_to_rm = '|'.join([id_col, time_col, target_col, 'cutoff', 'lo', 'hi'])\n",
    "    has_cutoff = 'cutoff' in df.columns\n",
    "    models = df.loc[:, ~df.columns.str.contains(cols_to_rm)].columns\n",
    "    str_models = ','.join([f\"{model}:double\" for model in models])\n",
    "    dtypes = df.dtypes\n",
    "    id_col_type = dtypes.loc[id_col]\n",
    "    if id_col_type == 'category':\n",
    "        raise NotImplementedError(\n",
    "            'Use of `category` type to identify each time series is not yet implemented. '\n",
    "            f'Please transform your {id_col} to string to continue.'\n",
    "        )\n",
    "    id_col_type = 'string' if id_col_type == 'object' else id_col_type\n",
    "    cutoff_col_type = ''\n",
    "    if has_cutoff:\n",
    "        cutoff_col_type = f\"{dtypes.loc['cutoff']}\".replace('64[ns]', '')\n",
    "    schema = (\n",
    "        f'{id_col}:{id_col_type},metric:string,'\n",
    "        + (f'cutoff:{cutoff_col_type},' if has_cutoff else '')\n",
    "        + str_models\n",
    "    )\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import test_eq\n",
    "from datasetsforecast.m4 import M4Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "fforma_url = 'https://github.com/Nixtla/m4-forecasts/raw/master/forecasts/submission-245.zip'\n",
    "M4Evaluation.load_benchmark('data', 'Hourly', fforma_url)\n",
    "eval_test = pd.read_csv('data/m4/datasets/submission-245.csv').query(\"id.str.startswith('H')\")\n",
    "eval_test = eval_test.set_index('id')\n",
    "eval_test.columns = list(range(1, 49))\n",
    "eval_test = eval_test.unstack().reset_index()\n",
    "eval_test.columns = ['ds', 'unique_id', 'FFORMA']\n",
    "eval_test = eval_test[['unique_id', 'ds', 'FFORMA']].sort_values(['unique_id', 'ds'])\n",
    "eval_test['y'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(\n",
    "    _schema_evaluation(eval_test),\n",
    "    'unique_id:string,metric:string,FFORMA:double'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def evaluate_forecasts(\n",
    "        df: pd.DataFrame,\n",
    "        metrics: List[Callable],\n",
    "        id_col: str = 'unique_id',\n",
    "        time_col: str = 'ds',\n",
    "        target_col: str = 'y',\n",
    "        level: Optional[List] = None,\n",
    "        agg_fn: Optional[Callable] = None,\n",
    "        agg_by: Optional[str] = None,\n",
    "        engine: Any = None,\n",
    "        kwargs_engine: Any = dict(),\n",
    "    ) -> pd.DataFrame:\n",
    "    evaluation_df = transform(\n",
    "        df, \n",
    "        _evaluate, \n",
    "        engine=engine, \n",
    "        params=dict(\n",
    "            metrics=metrics,\n",
    "            id_col=id_col,\n",
    "            time_col=time_col,\n",
    "            target_col=target_col,\n",
    "            level=level,\n",
    "        ), \n",
    "        schema=_schema_evaluation(\n",
    "            df, \n",
    "            id_col=id_col, \n",
    "            time_col=time_col, \n",
    "            target_col=target_col,\n",
    "        ), \n",
    "        partition=dict(by='unique_id')\n",
    "    )\n",
    "    return evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from datasetsforecast.losses import smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7x/1l3vkh3x4_q3s4r36b60s8f00000gn/T/ipykernel_32058/2552758337.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  evaluate_forecasts(eval_test, metrics=[smape]).groupby('metric').agg(np.mean)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FFORMA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>smape</th>\n",
       "      <td>199.969807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            FFORMA\n",
       "metric            \n",
       "smape   199.969807"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_forecasts(eval_test, metrics=[smape]).groupby('metric').agg(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
