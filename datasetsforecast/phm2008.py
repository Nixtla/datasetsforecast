# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/phm2008.ipynb.

# %% auto 0
__all__ = ['PHM2008Info', 'FD001', 'FD002', 'FD003', 'FD004', 'PHM2008']

# %% ../nbs/phm2008.ipynb 2
import os
from dataclasses import dataclass
from typing import Tuple

import numpy as np
import pandas as pd

from .utils import extract_file, download_file, Info

# %% ../nbs/phm2008.ipynb 3
@dataclass
class FD001:
    seasonality: int = 1
    horizon: int = 1
    freq: str = 'None'
    train_file: str = 'train_FD001.txt'
    test_file: str = 'test_FD001.txt'
    rul_file: str = 'RUL_FD001.txt'
    n_ts: int = 100
    n_test: int = 100

@dataclass
class FD002:
    seasonality: int = 1
    horizon: int = 1
    freq: str = 'None'
    train_file: str = 'train_FD002.txt'
    test_file: str = 'test_FD002.txt'
    rul_file: str = 'RUL_FD002.txt'
    n_ts: int = 260
    n_test: int = 259

@dataclass
class FD003:
    seasonality: int = 1
    horizon: int = 1
    freq: str = 'None'
    train_file: str = 'train_FD003.txt'
    test_file: str = 'test_FD003.txt'
    rul_file: str = 'RUL_FD003.txt'
    n_ts: int = 100
    n_test: int = 100

@dataclass
class FD004:
    seasonality: int = 1
    horizon: int = 8
    freq: str = 'None'
    train_file: str = 'train_FD004.txt'
    test_file: str = 'test_FD004.txt'
    rul_file: str = 'RUL_FD004.txt'
    n_ts: int = 249
    n_test: int = 248

# %% ../nbs/phm2008.ipynb 4
PHM2008Info = Info((FD001, FD002, FD003, FD004))

# %% ../nbs/phm2008.ipynb 5
@dataclass
class PHM2008:

    #source_url = 'https://forecasters.org/data/m3comp/M3C.xls'
    source_url = 'https://www.dropbox.com/s/1od45uh37tplxt7/CMAPSSData.zip?dl=1'

    @staticmethod
    def download(directory: str) -> None:
        """
        Download PHM2008 Dataset.
        
        Parameters
        ----------
        directory: str
            Directory path to download dataset.
        """
        path = f'{directory}/phm2008/'
        if not os.path.exists(path):
            download_file(path, PHM2008.source_url)
        if not os.path.isdir(f'{directory}/phm2008/CMAPSSData'):
            extract_file(f'{path}CMAPSSData.zip', path)

    @staticmethod
    def load(directory: str, group: str, clip_rul: bool=True) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Downloads and loads M3 data.

        Parameters
        ----------
        directory: str
            Directory where data will be downloaded.
        group: str
            Group name.
            Allowed groups: 'FD001', 'FD002', 'FD003', 'FD004'.
        clip_rul: bool
            Wether or not upper bound the remaining useful life to 125.

        Returns
        -------
        df: pd.DataFrame
            Target time series with columns ['unique_id', 'ds', 'y', 'exogenous'].
        """
        def _add_remaining_useful_life(df):
            # Get the total number of cycles for each unit
            grouped_by_unit = df.groupby(by="unit_nr")
            max_cycle = grouped_by_unit["time_cycles"].max()

            # Merge the max cycle back into the original frame
            result_frame = df.merge(max_cycle.to_frame(name='max_cycle'), left_on='unit_nr', right_index=True)

            # Calculate remaining useful life for each row (piece-wise Linear)
            remaining_useful_life = result_frame["max_cycle"] - result_frame["time_cycles"]
            result_frame["RUL"] = remaining_useful_life

            # drop max_cycle as it's no longer needed
            result_frame = result_frame.drop("max_cycle", axis=1)

            return result_frame

        PHM2008.download(directory)
        
        path = f'{directory}/phm2008/CMAPSSData'
        group = PHM2008Info.get_group(group)
        
        # define column names for easy indexing
        index_names = ['unit_nr', 'time_cycles']
        setting_names = ['setting_1', 'setting_2', 'setting_3']
        sensor_names = ['s_{}'.format(i) for i in range(1, 22)]
        col_names = index_names + setting_names + sensor_names

        # read data
        train = pd.read_csv((f'{path}/{group.train_file}'), 
                            sep='\s+', header=None, names=col_names)
        test = pd.read_csv((f'{path}/{group.test_file}'), 
                        sep='\s+', header=None, names=col_names)
        y_test = pd.read_csv((f'{path}/{group.rul_file}'), 
                            sep='\s+', header=None, names=['RUL'])
        

        # drop non-informative features in training set
        drop_sensors = ['s_1', 's_5', 's_6', 's_10', 's_16', 's_18', 's_19']
        drop_labels = setting_names + drop_sensors
        train.drop(labels=drop_labels, axis=1, inplace=True)

        # Add piece-wise target remaining useful life
        # in the paper the MAX RUL is mentioned as 125
        train = _add_remaining_useful_life(train)

        # Training set
        Y_train_df = train.rename(columns={'unit_nr': 'unique_id', 'time_cycles': 'ds',
                                           'RUL': 'y'})

        # drop non-informative features in testing set
        test.drop(labels=drop_labels, axis=1, inplace=True)

        # Testing set
        Y_test_df = test.rename(columns={'unit_nr': 'unique_id', 'time_cycles': 'ds'})
        
        # Only last RUL is available, complementing Y_test with linearly decreasing RUL
        count_ds = Y_test_df[['unique_id', 'ds']].groupby('unique_id').count().reset_index()
        ruls_list= []
        for index, row in count_ds.iterrows():
            ruls_list.append(np.arange(row.ds+y_test.RUL.values[index]-1, 
                                    y_test.RUL.values[index]-1, -1))
        ruls_list = np.concatenate(ruls_list)
        Y_test_df['y'] = ruls_list

        # Check that last RULs match
        last_rul = Y_test_df[['unique_id', 'y']].groupby('unique_id').last().reset_index()
        last_rul = last_rul['y'].values
        assert np.array_equal(last_rul, y_test.RUL.values)

        if clip_rul:
            Y_train_df['y'].clip(upper=125, inplace=True)
            Y_test_df['y'].clip(upper=125, inplace=True)

        return Y_train_df, Y_test_df
