# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/favorita.ipynb.

# %% auto 0
__all__ = ['FavoritaRawData', 'FavoritaData']

# %% ../nbs/favorita.ipynb 4
import os
import gc
import timeit
from typing import Tuple
from dataclasses import dataclass

from pathlib import Path
from itertools import chain

import numpy as np
import pandas as pd

# TODO: @kdgutier double check if it is possible to avoid scikit-learn dependency
# We are only using OneHotEncoder, and adds unnecesary dependencies' complexity.
import sklearn.preprocessing as preprocessing
from sklearn.preprocessing import OneHotEncoder

from .utils import download_file, extract_file, Info #, CodeTimer

# %% ../nbs/favorita.ipynb 7
# TODO: @kdgutier `CodeTimer`/`numpy_balance` are shared with hierarchicalforecast.utils
# In case of merging datasetsforecast/hierarchicalforeast we wil need to keep only one.
class CodeTimer:
    def __init__(self, name=None, verbose=True):
        self.name = " '"  + name + "'" if name else ''
        self.verbose = verbose

    def __enter__(self):
        self.start = timeit.default_timer()

    def __exit__(self, exc_type, exc_value, traceback):
        self.took = (timeit.default_timer() - self.start)
        if self.verbose:
            print('Code block' + self.name + \
                  ' took:\t{0:.5f}'.format(self.took) + ' seconds')

def numpy_balance(*arrs):
    """
    Fast NumPy implementation of 'balance' operation, useful to
    create a balanced panel dataset, ie a dataset with all the 
    interactions of 'unique_id' and 'ds'.

    **Parameters:**<br>
    `arrs`: NumPy arrays.<br>

    **Returns:**<br>
    `out`: NumPy array.
    """
    N = len(arrs)
    out =  np.transpose(np.meshgrid(*arrs, indexing='ij'),
                        np.roll(np.arange(N + 1), -1)).reshape(-1, N)
    return out

def numpy_ffill(arr):
    """
    Fast NumPy implementation of `ffill` that fills missing values
    in an array by propagating the last non-missing value forward.

    For example, if the array has the following values:<br>
    0  1  2    3<br>
    1  2  NaN  4<br>

    The `ffill` method would fill the missing values as follows:<br>
    0  1  2  3<br>
    1  2  2  4<br>

    **Parameters:**<br>
    `arr`: NumPy array.<br>

    **Returns:**<br>
    `out`: NumPy array.
    """
    # (n_series, n_dates) = arr.shape
    mask = np.isnan(arr)
    idx = np.where(~mask, np.arange(mask.shape[1]), 0)
    np.maximum.accumulate(idx, axis=1, out=idx)
    out = arr[np.arange(idx.shape[0])[:,None], idx]
    return out

def numpy_bfill(arr):
    """
    Fast NumPy implementation of `bfill` that fills missing values
    in an array by propagating the last non-missing value backwards.

    For example, if the array has the following values:<br>
    0  1  2    3<br>
    1  2  NaN  4<br>

    The `bfill` method would fill the missing values as follows:<br>
    0  1  2  3<br>
    1  2  4  4<br>
    
    **Parameters:**<br>
    `arr`: NumPy array.<br>

    **Returns:**
    `out`: NumPy array.
    """
    mask = np.isnan(arr)
    idx = np.where(~mask, np.arange(mask.shape[1]), mask.shape[1] - 1)
    idx = np.minimum.accumulate(idx[:, ::-1], axis=1)[:, ::-1]
    out = arr[np.arange(idx.shape[0])[:,None], idx]
    return out

# %% ../nbs/favorita.ipynb 12
def one_hot_encoding(df, index_col):
    """ 
    Encodes dataFrame `df`'s categorical variables skipping `index_col`.

    **Parameters:**<br>
    `df`: pd.DataFrame with categorical columns.<br>
    `index_col`: str, the index column to avoid encoding.<br>

    **Returns:**
    `one_hot_concat_df`: pd.DataFrame with one hot encoded categorical columns.<br>
    """
    encoder = OneHotEncoder()
    columns = list(df.columns)
    columns.remove(index_col)
    one_hot_concat_df = pd.DataFrame(df[index_col].values, columns=[index_col])
    for col in columns:
        dummy_columns = [f'{col}_[{x}]' for x in list(df[col].unique())]
        dummy_values  = encoder.fit_transform(df[col].values.reshape(-1,1)).toarray()
        one_hot_df    = pd.DataFrame(dummy_values, columns=dummy_columns)        
        one_hot_concat_df = pd.concat([one_hot_concat_df, one_hot_df], axis=1)
    return one_hot_concat_df

def nested_one_hot_encoding(df, index_col):
    """ 
    Encodes dataFrame `df`'s hierarchically-nested categorical variables skipping `index_col`.

    Nested categorical variables (example geographic levels country>state),
    require the dummy features to preserve encoding order, to reflect the hierarchy
    of the categorical variables.

    **Parameters:**<br>
    `df`: pd.DataFrame with hierarchically-nested categorical columns.<br>
    `index_col`: str, the index column to avoid encoding.<br>

    **Returns:**<br>
    `one_hot_concat_df`: pd.DataFrame with one hot encoded hierarchically-nested categorical columns.<br>
    """
    bottom_ids = list(df[index_col])
    del df[index_col]
    categories = [df[col].unique() for col in df.columns]
    encoder = OneHotEncoder(categories=categories,
                            sparse_output=False, dtype=np.float32)
    dummies = encoder.fit_transform(df)
    one_hot_concat_df = pd.DataFrame(dummies, index=bottom_ids,
                                     columns=list(chain(*categories)))
    return one_hot_concat_df

def get_levels_from_S_df(S_df):
    """ Get hierarchical index levels implied by aggregation constraints dataframe `S_df`.

    Create levels from summation matrix (base, bottom).
    Goes through the rows until all the bottom level series are 'covered'
    by the aggregation constraints to discover blocks/hierarchy levels.

    **Parameters:**<br>
    `S_df`: pd.DataFrame with summing matrix of size `(base, bottom)`, see [aggregate method](https://nixtla.github.io/hierarchicalforecast/utils.html#aggregate).<br>

    **Returns:**<br>
    `levels`: list, with hierarchical aggregation indexes, where each entry is a level.
    """
    cut_idxs, = np.where(S_df.sum(axis=1).cumsum() % S_df.shape[1] == 0.)
    levels = [S_df.iloc[(cut_idxs[i] + 1):(cut_idxs[i+1] + 1)].index.values for i in range(cut_idxs.size-1)]
    levels = [S_df.iloc[[0]].index.values] + levels
    assert sum([len(lv) for lv in levels]) == S_df.shape[0]
    return levels

# %% ../nbs/favorita.ipynb 16
# TODO: @kdgutier `make_holidays_distance_df` partially shared with neuralforecast.utils
# In particular some Transformers use a holiday-based global positional encoding.
# Same goes for HINT experiment that uses such general purpose holiday distances.
def distance_to_holiday(holiday_dates, dates):
    # Get holidays around dates
    dates = pd.DatetimeIndex(dates)
    dates_np = np.array(dates).astype('datetime64[D]')
    holiday_dates_np = np.array(pd.DatetimeIndex(holiday_dates)).astype('datetime64[D]')

    # Compute day distance to holiday
    distance = np.expand_dims(dates_np, axis=1) - np.expand_dims(holiday_dates_np, axis=0)
    distance = np.abs(distance)
    distance = np.min(distance, axis=1)
    
    # Convert to float
    distance = distance.astype(float)
    distance = distance * (distance>0)
    
    # Fix start and end of date range
    # TODO: Think better way of fixing absence of holiday
    # It seems that the holidays dataframe has missing holidays
    distance[distance>183] = 365 - distance[distance>183]
    distance = np.abs(distance)
    distance[distance>183] = 365 - distance[distance>183]
    distance = np.abs(distance)
    distance[distance>183] = 365 - distance[distance>183]
    distance = np.abs(distance)
    distance[distance>183] = 365 - distance[distance>183]    
    
    # Scale
    distance = (distance/183) - 0.5

    return distance

def make_holidays_distance_df(holidays_df, dates):
    #Make dataframe of distance in days to holidays
    #for holiday dates and date range
    distance_dict = {'date': dates}
    for holiday in holidays_df.description.unique():
        holiday_dates = holidays_df[holidays_df.description==holiday]['date']
        holiday_dates = holiday_dates.tolist()
        
        holiday_str = f'dist2_[{holiday}]'
        distance_dict[holiday_str] = distance_to_holiday(holiday_dates, dates)

    holidays_distance_df = pd.DataFrame(distance_dict)
    return holidays_distance_df

# %% ../nbs/favorita.ipynb 18
@dataclass
class Favorita200:
    freq: str = 'D'
    horizon: int = 34
    seasonality: int = 7
    test_size: int = 34
    tags_names: Tuple[str] = (
        'Country',
        'Country/State',
        'Country/State/City',
        'Country/State/City/Store',
    )

@dataclass
class Favorita500:
    freq: str = 'D'
    horizon: int = 34
    seasonality: int = 7
    test_size: int = 34
    tags_names: Tuple[str] = (
        'Country',
        'Country/State',
        'Country/State/City',
        'Country/State/City/Store',
    )

class FavoritaComplete:
    freq: str = 'D'
    horizon: int = 34
    seasonality: int = 7
    test_size: int = 34
    tags_names: Tuple[str] = (
        'Country',
        'Country/State',
        'Country/State/City',
        'Country/State/City/Store',
    )

FavoritaInfo = Info((Favorita200, Favorita500, FavoritaComplete))

# %% ../nbs/favorita.ipynb 20
class FavoritaRawData:
    """ Favorita Raw Data

    Raw subset datasets from the Favorita 2018 Kaggle competition.
    This class contains utilities to download, load and filter portions of the dataset.

    If you prefer, you can also download original dataset available from Kaggle directly.<br>
    `pip install kaggle --upgrade`<br>
    `kaggle competitions download -c favorita-grocery-sales-forecasting`
    """
    source_url = 'https://www.dropbox.com/s/xi019gtvdtmsj9j/favorita-grocery-sales-forecasting2.zip?dl=1'
    files = ['holidays_events.csv.zip', 'items.csv.zip', 'oil.csv.zip', 'sample_submission.csv.zip',
             'stores.csv.zip', 'test.csv.zip', 'train.csv.zip', 'transactions.csv.zip']

    @staticmethod
    def unzip(path):
        # Unzip Load, Price, Solar and Wind data
        # shutil.register_unpack_format('7zip', ['.7z'], unpack_7zarchive)
        for file in FavoritaRawData.files:
            filepath = f'{path}/{file}'
            extract_file(filepath, path)

    @staticmethod
    def download(directory: str) -> None:
        """Downloads Favorita Competition Dataset.
        The dataset weights 980MB, its download is not currently robust to
        brief interruptions of the process. It is recommended execute with
        good connection.
        """
        if not os.path.exists(directory):
            download_file(directory, FavoritaRawData.source_url, decompress=True)
        if not os.path.exists(f'{directory}/train.csv'):
            FavoritaRawData.unzip(directory)

    @staticmethod
    def _read_raw_data(directory):
        # Download Favorita Kaggle competition and unzip
        FavoritaRawData.download(directory)

        # We avoid memory-intensive task of infering dtypes
        dtypes_dict = {'id': 'int32',
                      'date': 'str',
                      'item_nbr': 'int32',
                      'store_nbr': 'int8', # there are only 54 stores
                      'unit_sales': 'float64', # values beyond are f32 outliers
                      'onpromotion': 'float64'}

        # We read once from csv then from feather (much faster)
        if not os.path.exists(f'{directory}/train.feather'):
            train_df = pd.read_csv(f'{directory}/train.csv',
                                  dtype=dtypes_dict,
                                  parse_dates=['date'])
            del train_df['id']
            train_df.reset_index(drop=True, inplace=True)
            train_df.to_feather(f'{directory}/train.feather')
            print("saved train.csv to train.feather for fast access")

        items = pd.read_csv(f'{directory}/items.csv')
        store_info = pd.read_csv(f'{directory}/stores.csv')
        store_info['country'] = 'Ecuador'

        # Change dtype for faster categorical wrangling
        items['class'] = items['class'].astype('category')
        items['family'] = items['family'].astype('category')        
        for col in ['country', 'state', 'city', 'store_nbr']:
            store_info[col] = store_info[col].astype('category')

        # Test is avoided because y_true is unavailable
        temporal = pd.read_feather(f'{directory}/train.feather')
        test = pd.read_csv(f'{directory}/test.csv', parse_dates=['date'])
        oil = pd.read_csv(f'{directory}/oil.csv', parse_dates=['date'])
        holidays = pd.read_csv(f'{directory}/holidays_events.csv', parse_dates=['date'])
        transactions = pd.read_csv(f'{directory}/transactions.csv', parse_dates=['date'])

        temporal['open'] = 1
        temporal['open'] = temporal['open'].astype('float32')

        return temporal, oil, items, store_info, holidays, transactions, test

    @staticmethod
    def _load_raw_group_data(directory, group, verbose=False):
        """ Load raw group data.

        Reads, filters and sorts Favorita subset dataset.

        **Parameters:**<br>
        `directory`: str, Directory where data will be downloaded.<br>
        `group`: str, dataset group name in 'Favorita200', 'Favorita500', 'FavoritaComplete'.<br>
        `verbose`: bool=False, wether or not print partial outputs.<br>

        **Returns:**<br>
        `filter_items`: ordered list with unique items identifiers in the Favorita subset.<br>
        `filter_stores`: ordered list with unique store identifiers in the Favorita subset.<br>
        `filter_dates`: ordered list with dates in the Favorita subset.<br>
        `raw_group_data`: dictionary with original raw Favorita pd.DataFrames, 
        temporal, oil, items, store_info, holidays, transactions. <br>
        """
        if group not in FavoritaInfo.groups:
            raise Exception(f'group not found {group}, select from Favorita200, Favorita500, FavoritaComplete')

        with CodeTimer('Read  ', verbose):
            temporal, oil, items, store_info, holidays, transactions, test \
                                                    = FavoritaRawData._read_raw_data(directory=directory)
        with CodeTimer('Filter', verbose):
            # https://arxiv.org/pdf/2106.07630.pdf reported 1687 vs 1688 days in our wrangling
            # we follow https://arxiv.org/abs/2110.13179 that keeps 2017> dates
            #date_range = pd.date_range(start_date, end_date, freq='D') 
            #print('len(date_range)', len(date_range))    

            temporal_dates  = temporal['date'].unique() # 1684 days
            start_date = '2017-01-01' # min(temporal_dates)
            end_date = max(temporal_dates)
            #end_date = '2017-08-31'  # Last date for test in Kaggle competition

            catalog_items   = set(items['item_nbr'].unique())
            catalog_stores  = set(store_info['store_nbr'].unique())
            catalog_dates   = pd.date_range(start=start_date, end=end_date, freq='D')
            catalog_dates   = set(catalog_dates.values.astype('datetime64[ns]'))

            temporal_dates  = set(temporal_dates)
            temporal_items  = set(temporal['item_nbr'].unique())
            temporal_stores = set(temporal['store_nbr'].unique())

            filter_dates = list(catalog_dates)
            filter_items  = list(catalog_items.intersection(temporal_items))
            filter_stores = list(catalog_stores.intersection(temporal_stores))

            if group=='Favorita200':
                #filter_items = filter_items[:200]
                np.random.seed(1)
                filter_items = np.random.choice(filter_items, size=200, replace=False)

            elif group=='Favorita500':
                #filter_items = filter_items[:500]
                np.random.seed(1)
                filter_items = np.random.choice(filter_items, size=500, replace=False)

            filter_items.sort()
            filter_stores.sort()
            filter_dates.sort()

            # Filter
            oil          = oil[(oil['date'] >= start_date) & (oil['date'] < end_date)]
            items        = items[items.item_nbr.isin(filter_items)]
            store_info   = store_info[store_info.store_nbr.isin(filter_stores)]
            holidays     = holidays[(holidays['date'] >= start_date) & (holidays['date'] < end_date)]
            transactions = transactions[(transactions['date'] >= start_date) & (transactions['date'] < end_date)]
            transactions = transactions[transactions.store_nbr.isin(filter_stores)]

            temporal     = temporal[temporal.item_nbr.isin(filter_items)]
            temporal     = temporal[temporal.store_nbr.isin(filter_stores)]

        with CodeTimer('Sort  ', verbose):
            # new sorted by hierarchy store_nbr for R benchmarks
            store_info = store_info.sort_values(by=['state', 'city', 'store_nbr'])
            store_info['new_store_nbr'] = np.arange(len(store_info))

            # Share the new store id with temporal and transactions data
            new_store_nbrs = store_info[['store_nbr', 'new_store_nbr']]
            transactions   = transactions.merge(new_store_nbrs, on=['store_nbr'], how='left')
            temporal       = temporal.merge(new_store_nbrs, on=['store_nbr'], how='left')

            # Overwrite the store_nbr ids
            del temporal['store_nbr'], transactions['store_nbr'], store_info['store_nbr']
            temporal['store_nbr']     = temporal['new_store_nbr']
            transactions['store_nbr'] = transactions['new_store_nbr']
            store_info['store_nbr']   = store_info['new_store_nbr']
            del temporal['new_store_nbr'], transactions['new_store_nbr'], store_info['new_store_nbr']

            # Final Sort
            temporal     = temporal.sort_values(by=['item_nbr', 'store_nbr', 'date'])
            oil          = oil.sort_values(by=['date'])
            items        = items.sort_values(by=['item_nbr'])
            store_info   = store_info.sort_values(by=['store_nbr'])
            transactions = transactions.sort_values(by=['store_nbr', 'date'])

        raw_group_data = dict(
            temporal=temporal, 
            oil=oil, 
            items=items, 
            store_info=store_info, 
            holidays=holidays, 
            transactions=transactions, 
            test=test,
        )

        return filter_items, filter_stores, filter_dates, raw_group_data

# %% ../nbs/favorita.ipynb 26
class FavoritaData:
    """ Favorita Data

    The processed Favorita dataset of grocery contains item sales daily history with additional
    information on promotions, items, stores, and holidays, containing 371,312 series from 
    January 2013 to August 2017, with a geographic hierarchy of states, cities, and stores. 
    This wrangling matches that of the DPMN paper.

    - [Kin G. Olivares, O. Nganba Meetei, Ruijun Ma, Rohan Reddy, Mengfei Cao, Lee Dicker (2022)."Probabilistic Hierarchical Forecasting with Deep Poisson Mixtures". International Journal Forecasting, special issue.](https://doi.org/10.1016/j.ijforecast.2023.04.007)
    """
    @staticmethod
    def _get_static_data(filter_items, filter_stores, items, store_info, temporal, verbose=False):
        with CodeTimer('static_bottom', verbose):
            # Create balanced item x store interaction
            balanced_prod = numpy_balance(filter_items, filter_stores)
            item_store_df = pd.DataFrame(balanced_prod, columns=['item_nbr', 'store_nbr'])
            item_store_df['unique_id'] = np.arange(len(item_store_df))

            # Create dummy variable for original vs series introduced by
            # balance procedure and the main unique_id index
            idxs = temporal[['item_nbr', 'store_nbr']].values
            unique_idxs = np.unique(idxs, axis = 0)

            original_series_df = pd.DataFrame(unique_idxs,
                                              columns=['item_nbr', 'store_nbr'])
            original_series_df['is_original'] = 1
            item_store_df = item_store_df.merge(original_series_df,
                                                on=['item_nbr', 'store_nbr'], how='left')
            item_store_df['is_original'] = item_store_df['is_original'].fillna(0)

            # Regional Static Variables
            # Adding prefix to avoid categorical hash collishion
            hier_df = store_info[['store_nbr', 'country', 'state', 'city']].copy()
            hier_df['state'] = 'state_['+ hier_df['state'].astype(str) + ']'
            hier_df['city'] = 'city_['+ hier_df['city'].astype(str) + ']'
            static_bottom = nested_one_hot_encoding(hier_df, index_col='store_nbr')
            Agg = static_bottom.values.T
            S = np.concatenate([Agg, np.eye(len(filter_stores), dtype=np.float32)], axis=0)
            
            filter_stores_str = [f'store_[{store}]' for store in filter_stores]
            S_df = pd.DataFrame(S, columns=filter_stores_str,
                                index=list(static_bottom.columns)+filter_stores_str)

            # Visualize geographic hierarchical aggregation matrix
            if verbose:
                plt.figure(num=1, figsize=(7, 3), dpi=80, facecolor='w')
                plt.spy(Agg.T)
                plt.show()

            static_bottom_columns = static_bottom.columns
            static_bottom = np.expand_dims(static_bottom, axis=0)
            static_bottom = np.repeat(static_bottom, repeats=len(filter_items), axis=0)
            static_bottom = static_bottom.reshape(-1, static_bottom.shape[-1])
            static_bottom = pd.DataFrame(static_bottom, columns=static_bottom_columns)

            static_items_bottom  = np.repeat(np.array(filter_items), len(filter_stores))
            static_stores_bottom = np.tile(np.array(filter_stores), len(filter_items))
            static_bottom['item_nbr']  = static_items_bottom
            static_bottom['store_nbr'] = static_stores_bottom

            static_bottom = static_bottom.merge(item_store_df,
                                            on=['item_nbr', 'store_nbr'], how='left')

        with CodeTimer('static_agg', verbose):
            static_agg = one_hot_encoding(items, index_col='item_nbr')

            if verbose:
                plt.figure(num=1, figsize=(7, 3), dpi=80, facecolor='w')
                plt.spy(static_agg.values.T)
                plt.show()

            # Add weight for loss perishable 1.25 and normal 1.0
            # https://www.kaggle.com/c/favorita-grocery-sales-forecasting/overview/evaluation
            static_agg['prob'] = np.ones(len(static_agg)) + 0.25 * static_agg["perishable_[1]"]
            static_bottom['prob'] = np.repeat(static_agg['prob'].values, len(filter_stores))

        return S_df, item_store_df, static_agg, static_bottom

    @staticmethod
    def _get_temporal_bottom(temporal, item_store_df, filter_dates, verbose=False):
        with CodeTimer('temporal_bottom', verbose):
            #-------------------- with CodeTimer('Temporal Balance') --------------------#
            # Two stage fast numpy balance, 
            # for memory and computational efficiency
            #item_nbrs  = temporal['item_nbr'].unique()
            #store_nbrs = temporal['store_nbr'].unique()
            #dates      = temporal['date'].unique()

            n_items  = len(item_store_df['item_nbr'].unique())
            n_stores = len(item_store_df['store_nbr'].unique())
            n_dates  = len(filter_dates)

            unique_ids = item_store_df['unique_id'].values
            balanced_prod = numpy_balance(unique_ids, filter_dates)
            balanced_df   = pd.DataFrame(balanced_prod, columns=['unique_id', 'date'])
            balanced_df['date'] = balanced_df['date'].astype(temporal['date'].dtype)

            zfill_cols  = ['unit_sales'] #, 'open'] # OPEN VARIABLE FROM TFT-Google IS BAAAD
            ffill_cols  = ['onpromotion']
            filter_cols = ['item_nbr', 'store_nbr', 'date']
            filter_cols = filter_cols + zfill_cols + ffill_cols
            temporal_df = temporal.filter(items=filter_cols)

            #-------------------- with CodeTimer('Temporal Merge'): --------------------#
            # Two stage merge with balanced data
            item_store_df = item_store_df[['unique_id', 'item_nbr', 'store_nbr', 'is_original']]
            item_store_df.set_index(['unique_id'], inplace=True)
            balanced_df.set_index(['unique_id'], inplace=True)
            balanced_df = balanced_df.merge(item_store_df, how='left',
                                            left_on=['unique_id'],
                                            right_index=True).reset_index()
            #check_nans(balanced_df)
            item_store_df = item_store_df.reset_index()

            temporal_df.set_index(['item_nbr', 'store_nbr', 'date'], inplace=True)
            balanced_df.set_index(['item_nbr', 'store_nbr', 'date'], inplace=True)
            balanced_df = balanced_df.merge(temporal_df, how='left',
                                            left_on=['item_nbr', 'store_nbr', 'date'],
                                            right_index=True).reset_index()
            #check_nans(balanced_df)
            del temporal_df, balanced_prod
            gc.collect()

            #-------------------- with CodeTimer('ZFill Data'): --------------------#
            for col in zfill_cols:
                balanced_df[col] = balanced_df[col].fillna(0)
            #check_nans(balanced_df)

            #-------------------- with CodeTimer('FFill Data'): --------------------#
            for col in ffill_cols:
                # Fast numpy vectorized ffill, requires balanced dataframe
                col_values = balanced_df[col].astype('float32').values
                col_values = col_values.reshape(n_items * n_stores, n_dates)
                col_values = numpy_ffill(col_values)
                col_values = numpy_bfill(col_values)
                balanced_df[col] = col_values.flatten()
                balanced_df[col] = balanced_df[col].fillna(0)
            #check_nans(balanced_df)
        
        # Rename variables for StatsForecast/NeuralForecast compatibility
        balanced_df.rename(columns={"date": "ds", "unit_sales": "y"}, inplace=True)

        return balanced_df

    @staticmethod
    def _get_temporal_agg(filter_items, filter_stores, filter_dates,
                          oil, holidays, transactions,
                          temporal_bottom, verbose=False):

        # Copy to avoid overwriting original
        oil = oil.copy()
        holidays = holidays.copy()
        transactions = transactions.copy()

        with CodeTimer('temporal_agg', verbose):
            normalizer  = preprocessing.StandardScaler()

            #-------------------- with CodeTimer('1. Temporal'): --------------------#
            # National sales per item
            balanced_prod = numpy_balance(filter_items, filter_dates)
            balanced_df = pd.DataFrame(balanced_prod, columns=['item_nbr', 'date'])
            balanced_df['item_nbr'] = balanced_df['item_nbr'].astype(filter_items[0].dtype)
            balanced_df['date'] = balanced_df['date'].astype(filter_dates[0].dtype)

            # collapse store dimension -> national
            #unit_sales  = temporal_bottom[['unit_sales']].values
            unit_sales  = temporal_bottom[['y']].values
            unit_sales  = unit_sales.reshape(len(filter_items), len(filter_stores), len(filter_dates), 1)
            unit_sales  = np.sum(unit_sales, axis=1)
            balanced_df['y'] = unit_sales.reshape(-1, 1)

            temporal_agg = balanced_df

            #-------------------- with CodeTimer('2. Oil'): --------------------#
            balanced_df = pd.DataFrame({'date': filter_dates})
            balanced_df = balanced_df.merge(oil, on='date', how='left')
            #check_nans(balanced_df)

            balanced_df['dcoilwtico'] = balanced_df['dcoilwtico'].fillna(method='ffill')
            balanced_df['dcoilwtico'] = balanced_df['dcoilwtico'].fillna(method='bfill')

            #check_nans(balanced_df)
            oil_agg = balanced_df
            oil_agg['dcoilwtico'] = normalizer.fit_transform(oil_agg['dcoilwtico'].values[:,None])

            if verbose:
                plt.figure(num=1, figsize=(7, 3), dpi=80, facecolor='w')
                plt.plot(balanced_df['date'], balanced_df['dcoilwtico'])
                plt.grid()
                plt.ylabel('Oil Price')
                plt.xlabel('Date')
                plt.show()
                plt.close()

            #-------------------- with CodeTimer('3. Holidays'): --------------------#
            # Calendar Variables
            calendar = pd.DataFrame({'date': filter_dates})
            calendar['day_of_week']  = calendar['date'].dt.dayofweek
            calendar['day_of_month'] = calendar['date'].dt.day
            calendar['month']        = calendar['date'].dt.month

            calendar['day_of_week']  = calendar['day_of_week'].astype('float64')
            calendar['day_of_month'] = calendar['day_of_month'].astype('float64')
            calendar['month']        = calendar['month'].astype('float64')

            calendar['day_of_week']  = normalizer.fit_transform(calendar['day_of_week'].values[:,None])
            calendar['day_of_month'] = normalizer.fit_transform(calendar['day_of_month'].values[:,None])
            calendar['month'] = normalizer.fit_transform(calendar['month'].values[:,None])

            if verbose:
                plt.figure(num=1, figsize=(7, 3), dpi=80, facecolor='w')
                plt.plot(calendar['day_of_week'], label='day_of_week')
                plt.plot(calendar['day_of_month'], label='day_of_month')
                plt.plot(calendar['month'], label='month')
                plt.legend()
                plt.grid()
                plt.ylabel('Calendar Variables')
                plt.xlabel('Date')
                plt.show()
                plt.close()

            # Holiday variables
            hdays = holidays[holidays['transferred']==False].copy()
            hdays.rename(columns={'type': 'holiday_type'}, inplace=True)

            national_hdays = hdays[hdays['locale']=='National']    
            national_hdays = national_hdays[national_hdays.holiday_type.isin(['Holiday', 'Transfer'])]
            national_hdays = make_holidays_distance_df(national_hdays, filter_dates)

            calendar_agg = calendar.merge(national_hdays, on=['date'], how='left')

            if verbose:
                # Plot to see calendar variables, depending on filter_dates some holidays are missing
                print('calendar_agg.columns: \n', calendar_agg.columns)
                # plt.plot(calendar_agg['dist2_[Navidad]'], label='Christmas')
                # plt.plot(calendar_agg['dist2_[Independencia de Cuenca]'], label='Independence')
                # plt.plot(calendar_agg['dist2_[Primer dia del ano]'], label='New Year')
                # plt.legend()
                # plt.grid()
                # plt.ylabel('Holiday Distance (days)')
                # plt.xlabel('Date')
                # plt.show()
                # plt.close()

            #-------------------- with CodeTimer('4. Transactions'): --------------------#
            # 'Transactions Balance'
            # Fast numpy balance
            balanced_prod = numpy_balance(filter_stores, filter_dates)
            balanced_df   = pd.DataFrame(balanced_prod, columns=['store_nbr', 'date'])
            balanced_df['date'] = balanced_df['date'].astype('datetime64[ns]')

            # 'Transactions Merge'
            # Merge with balanced data
            transactions.set_index(['store_nbr', 'date'], inplace=True)
            balanced_df.set_index(['store_nbr', 'date'], inplace=True)
            transactions = balanced_df.merge(transactions, how='left',
                                             left_on=['store_nbr', 'date'],
                                             right_index=True).reset_index()
            #check_nans(transactions)

            transactions = transactions.sort_values(by=['store_nbr', 'date'])
            trans_values = transactions.transactions.values
            
            trans_values = trans_values.reshape(len(filter_stores), len(filter_dates))
            trans_values = numpy_ffill(trans_values)
            trans_values = np.nan_to_num(trans_values)
            trans_values = trans_values.T
            trans_values = trans_values > 0
            trans_columns = [f'transactions_store_[{x}]' for x in filter_stores]

            transactions_agg = pd.DataFrame(trans_values, columns=trans_columns)
            transactions_agg['date'] = filter_dates

            # for x in STORES:
            #     transactions_agg[f'transactions_store_[{x}]'] = \
            #           normalizer.fit_transform(transactions_agg[f'transactions_store_[{x}]'].values[:,None])

            if verbose:
                plt.figure(num=1, figsize=(7, 3), dpi=80, facecolor='w')
                plt.plot(transactions_agg['transactions_store_[45]'], label='transactions_store_[45]')
                plt.plot(transactions_agg['transactions_store_[52]'], label='transactions_store_[52]')
                plt.grid()
                plt.ylabel('Total store transactions')
                plt.xlabel('Date')
                plt.legend()
                plt.show()
                plt.close()

            del balanced_prod, balanced_df, oil, holidays, transactions
            gc.collect()

            #-------------------- with CodeTimer('5. temporal_agg'): --------------------#
            #print("1. temporal_agg.shape", temporal_agg.shape)
            #print("2. oil_agg.shape", oil_agg.shape)
            #print("3. calendar_agg.shape", calendar_agg.shape)
            #print("4. transactions_agg.shape", transactions_agg.shape)
            #print("\n\n")
            #print("1. temporal_agg.dtypes \n", temporal_agg.dtypes, "\n")
            #print("2. oil_agg.dtypes \n", oil_agg.dtypes, "\n")
            #print("3. calendar_agg.dtypes \n", calendar_agg.dtypes, "\n")
            #print("4. transactions_agg.dtypes \n", transactions_agg.dtypes, "\n")

            temporal_agg.set_index(['date'], inplace=True)
            oil_agg.set_index(['date'], inplace=True)
            calendar_agg.set_index(['date'], inplace=True)
            transactions_agg.set_index(['date'], inplace=True)

            # Compile national aggregated data
            temporal_agg = temporal_agg.merge(oil_agg, how='left', left_on=['date'],
                                        right_index=True)#.reset_index()
            temporal_agg = temporal_agg.merge(calendar_agg, how='left', left_on=['date'],
                                        right_index=True)#.reset_index()
            temporal_agg = temporal_agg.merge(transactions_agg, how='left', left_on=['date'],
                                        right_index=True)#.reset_index()
            temporal_agg = temporal_agg.reset_index()
            #check_nans(temporal_agg)

        # Rename variables for StatsForecast/NeuralForecast compatibility
        temporal_agg.rename(columns={"date": "ds", "unit_sales": "y"}, inplace=True)

        return temporal_agg

    @staticmethod
    def load_preprocessed(directory: str, group: str, cache: bool=True, verbose: bool=False) -> \
        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """ Load Favorita group datasets.

        For the exploration of more complex models, we make available the entire information
        including data at the bottom level of the items sold in Favorita stores, in addition
        to the aggregate/national level information for the items.

        **Parameters:**<br>
        `directory`: str, directory where data will be downloaded and saved.<br>
        `group`: str, dataset group name in 'Favorita200', 'Favorita500', 'FavoritaComplete'.<br>
        `cache`: bool=False, If `True` saves and loads.<br>
        `verbose`: bool=False, wether or not print partial outputs.<br>

        **Returns:**<br>
        `static_bottom`: pd.DataFrame, with static variables of bottom level series.<br>
        `static_agg`: pd.DataFrame, with static variables of aggregate level series.<br>
        `temporal_bottom`: pd.DataFrame, with temporal variables of bottom level series.<br>
        `temporal_agg`: pd.DataFrame, with temporal variables of aggregate level series.<br>
        """
        group_path = f'{directory}/{group}'

        if os.path.exists(group_path) and cache:
            if verbose: print('Read preprocessed data to avoid unnecesary computation')
            S_df = pd.read_csv(f'{group_path}/hier_constraints.csv', index_col=0)
            static_agg = pd.read_csv(f'{group_path}/static_agg.csv')
            static_bottom = pd.read_csv(f'{group_path}/static_bottom.feather')
            temporal_agg = pd.read_feather(f'{group_path}/temporal_agg.feather')
            temporal_bottom = pd.read_feather(f'{group_path}/temporal_bottom.feather')
            return static_agg, static_bottom, temporal_agg, temporal_bottom, S_df

        else:
            filter_items, filter_stores, filter_dates, raw_group_data = \
                FavoritaRawData._load_raw_group_data(directory=directory, group=group, verbose=verbose)

            S_df, item_store_df, static_agg, static_bottom = \
                              FavoritaData._get_static_data(filter_items=filter_items, 
                                                        filter_stores=filter_stores,
                                                        items=raw_group_data['items'], 
                                                        store_info=raw_group_data['store_info'], 
                                                        temporal=raw_group_data['temporal'], 
                                                        verbose=verbose)

            temporal_bottom = FavoritaData._get_temporal_bottom(temporal=raw_group_data['temporal'],
                                                        item_store_df=item_store_df,
                                                        filter_dates=filter_dates,
                                                        verbose=verbose)

            temporal_agg = FavoritaData._get_temporal_agg(filter_items=filter_items,
                                                        filter_stores=filter_stores,
                                                        filter_dates=filter_dates,
                                                        oil=raw_group_data['oil'],
                                                        holidays=raw_group_data['holidays'],
                                                        transactions=raw_group_data['transactions'],
                                                        temporal_bottom=temporal_bottom, verbose=verbose)
            
        del raw_group_data
        gc.collect()

        if not os.path.exists(group_path):
            os.makedirs(group_path)

        S_df.to_csv(f'{group_path}/hier_constraints.csv', index=True)
        item_store_df.to_csv(f'{group_path}/item_store.csv')

        static_agg.to_csv(f'{group_path}/static_agg.csv', index=False)
        static_bottom.to_csv(f'{group_path}/static_bottom.feather', index=False)

        temporal_bottom.to_feather(f'{group_path}/temporal_bottom.feather')
        temporal_agg.to_feather(f'{group_path}/temporal_agg.feather')

        return static_agg, static_bottom, temporal_agg, temporal_bottom, S_df
    
    @staticmethod
    def load(directory: str, group: str, cache: bool=True, verbose: bool=False):
        """
        Load Favorita forecasting benchmark dataset.

        In contrast with other hierarchical datasets, this dataset contains a geographic
        hierarchy for each individual grocery item series, identified with 'item_id' column.
        The geographic hierarchy is captured by the 'hier_id' column.

        For this reason minor wrangling is needed to adapt it for use with [`HierarchicalForecast`](https://github.com/Nixtla/hierarchicalforecast),
        and [`StatsForecast`](https://github.com/Nixtla/statsforecast) libraries.

        **Parameters:**<br>
        `directory`: str, directory where data will be downloaded and saved.<br>
        `group`: str, dataset group name in 'Favorita200', 'Favorita500', 'FavoritaComplete'.<br>
        `cache`: bool=False, If `True` saves and loads.<br>
        `verbose`: bool=False, wether or not print partial outputs.<br>

        **Returns:**<br>
        `Y_df`: pd.DataFrame, target base time series with columns ['item_id', 'hier_id', 'ds', 'y'].<br>
        `S_df`: pd.DataFrame, hierarchical constraints dataframe of size (base, bottom).<br>
        """
        # Load preprocessed data
        _, _, _, temporal_bottom, S_df = \
            FavoritaData.load_preprocessed(directory=directory, group=group,
                                           cache=cache, verbose=verbose)

        stores    = temporal_bottom.store_nbr.unique()
        items     = temporal_bottom.item_nbr.unique()
        dates     = temporal_bottom.ds.unique()

        cls_group = FavoritaInfo[group]
        tags = dict(zip(cls_group.tags_names, get_levels_from_S_df(S_df)))

        # Apply hierarchical aggregation
        # [n_items, n_stores, n_time] -> [n_items, n_hier, n_time]
        Y_bottom  = temporal_bottom['y'].values
        Y_bottom  = Y_bottom.reshape((len(items), len(stores), len(dates)))
        Y_hier = np.einsum('ist,sh->iht', Y_bottom, S_df.values.T)

        # Create hierarchical series dataframe
        item_id = np.repeat(items, len(S_df) * len(dates))
        hier_id = np.tile(np.repeat(S_df.index, len(dates)), len(items))
        ds = np.tile(np.tile(dates, len(S_df)), len(items))

        Y_df = pd.DataFrame(dict(
                item_id = item_id, hier_id = hier_id,
                ds = ds, y = Y_hier.flatten()))
        
        return Y_df, S_df, tags
