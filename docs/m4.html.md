---
title: M4
description: M4 dataset
---

##

::: datasetsforecast.m4.Other
    handler: python
    options:
      show_if_no_docstring: false

::: datasetsforecast.m4.Hourly
    handler: python
    options:
      show_if_no_docstring: false

::: datasetsforecast.m4.Daily
    handler: python
    options:
      show_if_no_docstring: false

::: datasetsforecast.m4.Weekly
    handler: python
    options:
      show_if_no_docstring: false

::: datasetsforecast.m4.Monthly
    handler: python
    options:
      show_if_no_docstring: false

::: datasetsforecast.m4.Quarterly
    handler: python
    options:
      show_if_no_docstring: false

::: datasetsforecast.m4.Yearly
    handler: python
    options:
      show_if_no_docstring: false

## Download data class

::: datasetsforecast.m4.M4
    handler: python
    options:
      show_if_no_docstring: false

## Evaluation class

::: datasetsforecast.m4.M4Evaluation
    handler: python
    options:
      show_if_no_docstring: false

### URL-based evaluation

The method `evaluate` from the class
[`M4Evaluation`](https://Nixtla.github.io/datasetsforecast/m4.html#m4evaluation)
can receive a url of a [benchmark uploaded to the M4
competiton](https://github.com/Mcompetitions/M4-methods/tree/master/Point%20Forecasts).

The results compared to the on-the-fly evaluation were obtained from the
[official
evaluation](https://github.com/Mcompetitions/M4-methods/blob/master/Evaluation%20and%20Ranks.xlsx).

```python
from fastcore.test import test_close
```

```python
esrnn_url = 'https://github.com/Nixtla/m4-forecasts/raw/master/forecasts/submission-118.zip'
esrnn_evaluation = M4Evaluation.evaluate('data', 'Hourly', esrnn_url)
# Test of the same evaluation as the original one
test_close(esrnn_evaluation['SMAPE'].item(), 9.328, eps=1e-3)
test_close(esrnn_evaluation['MASE'].item(), 0.893, eps=1e-3)
test_close(esrnn_evaluation['OWA'].item(), 0.440, eps=1e-3)
esrnn_evaluation
```

### Numpy-based evaluation

Also the method `evaluate` can recevie a numpy array of forecasts.

```python
fforma_url = 'https://github.com/Nixtla/m4-forecasts/raw/master/forecasts/submission-245.zip'
fforma_forecasts = M4Evaluation.load_benchmark('data', 'Hourly', fforma_url)
fforma_evaluation = M4Evaluation.evaluate('data', 'Hourly', fforma_forecasts)
# Test of the same evaluation as the original one
test_close(fforma_evaluation['SMAPE'].item(), 11.506, eps=1e-3)
test_close(fforma_evaluation['MASE'].item(), 0.819, eps=1e-3)
test_close(fforma_evaluation['OWA'].item(), 0.484, eps=1e-3)
fforma_evaluation
```
